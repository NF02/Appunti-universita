\chapter{Il determinante \label{ildet}}
In questo capitolo introdurremo uno strumento alternativo alla riduzione a gradini per determinare se le righe ({\it o le colonne}) di una matrice
siano dipendenti. Per poterne dare la definizione rigorosa, dobbiamo prima fare alcuni richiami sulle permutazioni.
\section {Richiami sulle permutazioni \label{ricsullperm}}
Dato l'insieme $\{1,2,\dots,n\}$ dei numeri naturali compresi tra 1 e $n$, per un certo $n$, una funzaione
da $\{1,2,\dots,n\}$ in se stesso associa a ogni elemento di $\{1,2,\dots,n\}$ un'immagine, scelta sempre
all'interno di $\{1,2,\dots,n\}$. Se facciamo in modo che le immagini siano tutte diverse senza
ripetizioni\footnote{\textit{Si dice la funzione è iniettiva:} una funzione iniettiva da un insieme finito in
  se stesso è automaticamente anche suriettiva, e quindi biiettiva. Richiameremo queste nozioni nel prossimo capitolo.},
queste ci daranno ancora tutti gli elementi $1,2,\dots,n$ semplicemente disposti in un altro ordine, ovvero permutati.
Si parla di {\em permutazione di n elementi.} Ad esempio, le seguenti rappresentano permutazioni di 4 elementi:
\begin{equation*}
  \begin{matrix}
    1 \to 1 & 1 \to 3\\
    2 \to 3 & 2 \to 4\\
    3 \to 2 & 3 \to 2\\
    4 \to 4 & 4 \to 1
  \end{matrix}
\end{equation*}
L'insieme delle permutazioni di $n$ elementi si denota $S_n$. Per ogni $n$, tale insieme contiene esattamente $n!:=n(n-1)(n-2)\dots 2*1$ (cioè {\it n fattoriale}) permutazioni: ad esempio, per $n = 2$ abbiamo $2!= 2 * 1 = 2$ permutazione possibili, ovvero
\begin{equation*}
  \begin{matrix}
    1 \to 1 & 1 \to 2\\
    2 \to 2 & 2 \to 1
  \end{matrix}
\end{equation*}
(tra le permutazioni vi è sempre anche anche quella che associa a ogni elemento se stesso, detta permutazione identica\footnote{Come funzione, si tratta della cosiddetta identità o funzione identica}).\\
Per $n=3$ abbiamo invece $3!=3*2*1=6$ permutazioni possibili, ovvero
\begin{equation*}
  \begin{matrix}
    1 \to 1 & 1 \to 2 & 1 \to 1 & 1 \to 3 & 1 \to 2 & 1 \to 3 \\
    2 \to 2 & 2 \to 1 & 2 \to 3 & 2 \to 2 & 2 \to 3 & 2 \to 1 \\
    3 \to 3 & 3 \to 3 & 3 \to 2 & 3 \to 1 & 3 \to 1 & 3 \to 2 \\
      p_1   &    p_2   &   p_3   &    p_4  &    p_5  &    p_6
  \end{matrix}
\end{equation*}
Si noti che $p_2, \text{ } p_3$ e $p_4$ scambiano trra loro due elementi lasciando fisso il terzo ($p_2$ scambia tra loro 1 e 2, $p_3$ scambia 2 e 3): in generale, una permutazione di questo tipo, che scambia tra loro è una trasposizione anche la prima permutazione di 4 elementi presentata all'inizio del paragrafo ({\tt scambia tra loro 2 e 3 lasciando fissi 1 e 4}), mentre la seconda non lo è.\\
Benché non tutte le permutazioni siano trasposizioni, si può dimostrare che qualunque permutazione può essere realizzata eseguendo una sequenza di trasposizioni. Ad esempio, la permutazione $p_5$ di sopra, che non è una trasposizione, può tuttavia essere ottenuta scambiando prima 1 e 2, e poi 1 e 3:
\begin{equation*}
  \begin{matrix}
    1 \to 2 \to 2\\
    2 \to 1 \to 3\\
    3 \to 3 \to 1
  \end{matrix}
\end{equation*}
ovvero può essere ottenuta componendo 2 trasposizioni.\\
In generale, se il numero di trasposizioni che servono per ottenere una permutazione data $p$ è pari, si pari, si dice che $p$ è una {\em permutazione pari}; se invece il numero di trasposizioni che servono per ottenere $p$ è dispari, si dice che $p$ è una {\em permutazione dispari}. Ad esempio, $p_5$ è una permutazione pari, in quanto l'abbiamo ottenuta componendo 2 trasposizioni; è facile vedere che anche $p_6$ è una permutazione pari, in quanto può essere ottenuta componendo due trasposizioni:
\begin{equation*}
  \begin{matrix}
    1 \to 3 \to 3\\
    2 \to 3 \to 1\\
    3 \to 1 \to 2
  \end{matrix}
\end{equation*}
Chiaramente, se una permutazione è già essa una trasposizione, allora essa è dispori ({\tt 1 è un numero dispari}).\\
Si noti che possono esserci più modi diversi di decomporre una permutazione come composizione di trasposizioni, ad esempio, la permutazione identica può essere vista o come risultato di 0 trasposizioni, oppure come risultato di 2 trasposizioni, ad esempio
\begin{equation*}
  \begin{matrix}
    1 \to 2 \to 1\\
    2 \to 1 \to 2\\
    3 \to 3 \to 3
  \end{matrix}
\end{equation*}
Tuttavia, si pu`o dimostrare che il numero di trasposizioni che servono per ottenere una permutazione data `e o sempre pari o sempre dispari ({\it nell’esempio, 0 o 2, comunque pari}).\\
Si può allora definire il \textit{segno s(p) di una permutazione p} come $s(p)=+1$ se $p$ è una permutazione dispari.\\
Siamo ora pronti a definire il determinante.
\section{La definizione di determinante}
Sia $A$ una matrice che ha $n$ righe e $n$ colonne, per qualche $n>0$: tali matrici si dicono {\it quadrate} e il numero $n$ comune a roghe e colonne si dice {\it l'ordine della matrice.} Il determinante associa a ogni matrice $A$ quadrata di ordine $n$ a entrate in un campo $\mathds{K}$ un elemento $\det(A) \in \mathds{K}$, funzione delle sue entrate, per il quale vedremo che vale l'importante proprietà che $\det(A)=0$ se e solo se la matrice ha rango minore di $n$, ovvero se e solo se le righe ({\it o le colonne}) della matrice sono dipendenti.
\begin{definizione}
  Sia A una matrice quadrata di ordine $n$ con entrate $a_{ij}$. Allora
  \begin{equation}
	\det(A)=\sum_{p\in S_n}s(p)a_{1p(1)}a_{2p(n)}
  \end{equation}
  In altrea parole, il determinante di una matrice quadrata di ordine $n$ è dato da una sommatoria che ha addendo per ogni permutazione $p\in S_n$: ognuno di questi addendi è un prodotto di entrata di $A$ del tipo $a_{1p(1)}\text{ } a_{2p(2)} \text{ \dots } a_{np(n)},$ con davanti un segno + o - a seconda che la permutazione $p$ sia pari o dispari. Si noti che l'espressione $a_{1p(1)},\text{ } a_{2p(2)}, \text{ \dots }, a_{np(n)}$ è il prodotto di $n$ entrate scelte nella matrice, una per ogni riga, con gli indici di colonna dati da $p(1),\text{ } p(2),\dots, \text{ } p(n)$: poiché una permutazione scambia gli indici $1,\text{ }2,\dots, \text{ } n$ senza ripetizioni, stiamo praticamente scegliendo un'entrata da ogni riga in modo però che le entrate scelte stiano anche su colonne diverse.
\end{definizione}
Per chiarire e illustrare la definizione precedente, consideriamo in particolare i casi $n = 2$ e $n = 3$.\\
Sia $A=\begin{pmatrix}
         a_{11} & a_{12}\\
         a_{21} & a_{22}
       \end{pmatrix}$ una matrice quadrata di ordine $n=2$. Come abbiamo visto sopra ci sono solo due permutazioni dell'insieme $\{1,2\}$ ({\tt l'identità e la trasposizione che scambia 1 con 2}) quindi nella sommatoria avremo solo due addendi, del tipo $s(p) a_{1p(1)}a_{2p(2)}$: se $p$ è l'identità, che come abbiamo osservato sopra è un permutazione pari e quindi $s(p)=+1$ e l'addendo corrispondente sarà $+a_{11}a_{22}$: se $p$ è la trasposizione che scambia 1 con 2, che è una permutazione dispari, si ha $s(p)=-1$ e l'addendo corrispondente sarà $-a_{12}a_{21}$. Il determinante di una matrice quadrata $A$ di ordine 2 risulta quindi essere
       \begin{equation}
		\det(A)=a_{11}a_{22}-a_{12}a_{21}
       \end{equation}
       Nel caso di una matrice $A= \begin{pmatrix}
                                     a_{11} & a_{12} & a_{13}\\
                                     a_{21} & a_{22} & a_{23}\\
                                     a_{31} & a_{32} & a_{33}
                                   \end{pmatrix}$ quadrata di ordine $n = 3$, la sommatoria avrà 6 addendi, tanti quante sono le permutazioni dell'insieme $\{1,2,3\}$, e per ognuna di queste permutazioni $p$ l'addendo corrispondente sarà del tipo $s(p)a_{1p(1)}a_{2p(2)}a_{3p(3)}$. Più precisamente, avremo
\begin{itemize}
\item l'addendo $+a_{11}a_{12}a_{33}$ corrispondente alla permutazione $p(1) = 1, p(2) =2, p(3) =3$ ({\tt cioè la permutazione identica, che è una permutazione pari})
\item l'addendo $-a_{11}a_{23}a_{32}$ corrispondente alla permutazione $p(1) = 1, p(2) =3, p(3) =2$ ({\tt che è una trasposizione e quindi una permutazione dispari})
\item l'addendo $+a_{12}a_{23}a_{31}$ corrispondente alla permutazione $p(1)=2,\text{ }p(2)=3,\text{ }p(3)=1$ ({\tt che è una trasposizione dispari})
\item l'addendo $-a_{12}a_{21}a_{33}$ corrispondente alla permutazione $p(1)=2,\text{ }p(2)=1,\text{ } p(3)=3$ ({\tt che è una teasposizione e quindi una permutazione dispari})
\item l'addento $+a_{13}a_{21}a_{32}$ corrispondente alla permutazione $p(1)=3,\text{ } p(2)=1,\text{ } p(3)=2$ ({\tt che si può scrivere come composizione di due trasposizioni ed è quindi una permutazione pari})
\item l'addendo $-a_{13}a_{22}a_{31}$ corrispondente alla permutazione $p(1) = 3,\text{ }p(2) = 2,\text{ }p(3)=1$ ({\tt che è una trasposizione e quindi una permutazione dispari})
\end{itemize}
e quindi si avrà, per una matrice $A$ di ordine 3: 
\begin{equation}
	\det(A)=a_{11}a_{12}a_{33}-a_{11}a_{23}a_{32}+a_{12}a_{23}a_{31}-a_{12}a_{21}a_{33}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}
\end{equation}
Ora, vedremo nel Paragrafo la dimostrazione del fatto che il determinante di una matrice quadratea di ordine $n$ si annulla se e solo se la matrice ha $n=2$ e $n=3$, usando le farmule esplicite ({\tt 3.2}) e ({\tt 3.3}).\\
Nel caso di una matrice $A = \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}$ di ordine 2, essendoci solo proporzionali, ovvero diciamo che esiste un $c \in \mathds{K}$ tale che $(a_{11},a_{12})=c(a_{21}a_{22})$, cioè
\begin{equation}
	a_{11}=ca_{21},\text{ }a_{12}=ca_{22}
\end{equation}
Ma allora, moltiplicando ({\tt a entrambi i membri}) la prima uguaglianza per $a_{22}$ e la seconda per $21$ si ha $a_{11}a_{22}=ca_{21}a_{22}$ e $a_{12}a_{21}=ca_{21}a_{21}$, da cui vediamo che $a_{11}a_{22}=ca_{21}a_{22}$: quindi $a_{11}a_{22}-a_{12}a_{21}=0$, ovvero $\det(A)=0$. Quindi se una matrice di ordine 2 ha le righe proporzionali, il suo determinante è zero.\\
Viceversa, supponiamo che il determinante $a_{11}a_{22}-a_{12}a_{21}$ sia zero, ovvero
\begin{equation}
	a_{11}a_{22}=a_{12}a_{21}
\end{equation}
Supponendo per il momento che le entrate $a_{21},a_{22}$ della seconda riga non siano nulle, dividendo entrambi i membri della (3.5) per $a_{21}$ e $a_{22}$  otteniamo
\begin{equation}
	\frac{a_{11}}{a_{21}}=\frac{a_{12}}{a_{22}}
\end{equation}
cioè si ha $\frac{a_{11}}{a_{21}}=c$ e $\frac{a_{12}}{a_{22}}=c$ per qualche $c\to \mathds{K}$, da cui ritroviamo le uguaglianze ({\tt 3.4}), che $a_{21}$ e $a_{22}$ fossero entrambi diversi da zero: infatti, se ad esempio $a_{21}=0$ allora la (3.5) diventa $a_{11}a_{22}=0$, che equivale a dire che o $a_{11}=0$ oppure $a_{22}=0$: ma nel primo caso la matrice è della forma $A=\begin{pmatrix} 0 & a_{12} \\ 0 & a_{22}\end{pmatrix}$, nel secondo $A=\begin{pmatrix} a_{11} & a_{12} \\ 0 & 0\end{pmatrix}$, e in entrambi i casi otteniamo sempre che le righe sono proporzionali.\\
Nel caso di matrici di ordine 3, dimostrare che il determinante si annulla {\em se e solo se} le righe sono dipendenti, non p difficile se ci aiutiamo con un'interpretazione geometrica: {\bf infatti, prendiamo l'espressione (3.3) del determinante di una tale matrice e riscriviamola nel modo seguente:}
\begin{equation}
	a_{11}(a_{22}a_{33}-a_{23}a_{32})+ a_{12}(a_{23}a_{31}-a_{21}a_{33})+a_{13}(a_{21}a_{32}-a_{22}a_{31})
\end{equation}
Ora, le tre quantità dentro le parentesi tonde non sono altro che le componenti del prodotto vettoriale dei due vettori di $\mathds{R}^3$ dati dalla seconda e dalla terza riga della matrice:
\begin{equation*}
	R_2\wedge R_3=(a_{21},a_{22},a_{23})\wedge(a_{31},a_{32},a_{33})=(a_{22}a_{33}-a_{23}a_{32},a_{23}a_{33}-a_{21}a_{33},_{21},a_{32}-a_{22}a_{31})
\end{equation*}
Quindi la (3.7), dal momento che moltiblica le componenti di $R_1=(a_{11},a_{12},a_{13})$ con le rispettive componenti di $R_2\wedge R_3$ ({\tt la prima con la prima, la seconda con la seconda e la terza con la terza}) e somma, non è nient'altro che quello che abbiamo chiamato prodotto scalare tra $R_1$ e $R_2 \wedge R_3$. Quindi, dire che il determinante di una matrice di ordine 3 si annulla quivale a dire che le sue tre righe $R_1,R_2,R_3$ verificano l'uguaglianza.
\begin{equation}
	R_1*(R_2\wedge R_3)=0
\end{equation}
Ora, se fissiamo nello spazio $V^{3}_O$ dei vettori applicati nello spazio tridimensionali, le terne $R_1, R_2, R_3$ rappresenterano le coordinate di tre vettori applicati $\vec{OP_1}, \vec{OP_2}, \vec{OP_3}$ rispettivamente, e la (3.8) ci sta allora dicendo che il vettore $\vec{OP_1}$ è perpendicolare al vettore $\vec{OP_2}\wedge \vec{OP_3}$ ({\it infatti, abbiamo dimostrato che due vettori dello spazio sono perperndicolari solo se il prodotto scalare tra le terne dele lorfo coordinate è zero}). Ma, essendo a sua volta $\vec{OP_2}\wedge \vec{OP_3}$, per le proprietà del prodotto vettoriale, perpendicolare $\vec{OP_2}\wedge \vec{OP_3}$ significa dire che anche $\vec{OP_1}$ deve stare sul piano $p$:

\[\begin{tikzcd}
	&&& {\vec{OP_2}\wedge\vec{OP_3}} \\
	\\
	& \bullet &&&&&&&&& \bullet \\
	&&&&&& {P_3} &&&&& {\textit{p}} \\
	&&& \bullet && {} &&& {P_2} \\
	&&&&&&& {P_1} \\
	\bullet &&&&&&&&& \bullet
	\arrow[no head, from=7-1, to=3-2]
	\arrow[no head, from=7-1, to=7-10]
	\arrow[no head, from=7-10, to=3-11]
	\arrow[from=5-4, to=1-4]
	\arrow[from=5-4, to=6-8]
	\arrow[from=5-4, to=5-9]
	\arrow[from=5-4, to=4-7]
	\arrow[no head, from=3-2, to=3-11]
\end{tikzcd}\]
Quindi i tre vettori, stando sullo stesso piano, sono tra loro dipendenti, e
concludiamo che sono dipendenti anche le terne $R_1, R_2, R_3$ delle loro coordinate\footnote{Ricordiamo che dei vettori in uno spazio vettoriale sono dipendenti se e solo se lo sono, le \textit{n-uple} delle loro coordinate rispetto a una base scelta.}. Questo mostra come volevamo che il determinante di una matrice di ordine 3 si annulla se e solo se le sue riche $R_1, R_2,R_3$ sono dipendenti.\\
Nel paragrafo sulle proprietà del determinante, dimostreremo in modo puramente algebrico che questo fatto è vero per matrici di qualunque ordine. Prima di fare ciò, vediamo nel prossimo paragrafo un modo di calcolkare il determinante alternativo alla definizione, il cui utilizzo diretto richiederebbe di scrivere una sommaria che per una matrice di ordine $n$ ha $n!$ addendi, tanti quanti le permutazioni di $n$ elementi (si pensi che già per $n=4$ abbiamo $4!=24$ addendi).
\section{La formula di Laplace \label{Laplace}}
Allo scopo di calcolare il determinanete, useremo la cosiddetta \textit{formula di Laplace}, per scrivere la quale abbiamo prime bisogno di dare la seguante
\begin{definizione}
Sia $A$ una matrice quadrata di ordine $n$: per ogni entrata $a_{ij}$ di $A$, definiamo il \textit{cofattore di $a_{ij}$}, denotato $C_{ij}$, come il determinante della matrice di ordine $n-1$ che si ottiene da $A$ cancellando la riga $i$ e la colonna $j$, moltiplicato per $(-1)^{i+j}$
\end{definizione}
Vediamo subito un esempio: consideriamo la matrice di ordine 3 seguente
\begin{equation*}
  \begin{pmatrix}
    1 & 2 & 4 \\
    4 & 5 & 6 \\
    7 & 8 & 9
  \end{pmatrix}
\end{equation*}
L'entrata $a_{11}$ è uguale a 1: il suo cofattore $C_{11}$ è, in base alla definizione <++>, il determinante della matrice di ordine 2 che si ottiene cancellando la prima riga e la prima colonna (ovviamente la riga e la colonna dell-entrata $a_{11}$ presa in considerazione) moltiplicato per $(-1)^{1+1}$:
\begin{equation*}
	C_{11}=(-1)^{1+1}\det\begin{pmatrix} 5 & 6\\ 8 & 9 \end{pmatrix} = (-1)^2(5*9-6*8)=+(45-48)=-3
\end{equation*}
Invece, se per esempio prendiamo l'entrata $a_{23}=6$, il suo cofattore $C_{23}$ è il determinante della matrice di ordine 2 che si ottiene cancellando la seconda riga e la terza colonna (ovvero la riga e la colonna dell'entrata $(-1)^{2+3}$):
\begin{equation*}
	C_{11}=(-1)^{1+1}\det\begin{pmatrix} 1 & 2 \\ 7 & 8 \end{pmatrix} = (-1)^5(1*8-2*7)=-(8-14)=+6
\end{equation*}
Siamo ora pronti a enunciare il seguante risultato, sche ci dà una formula per il calcolo del determinante di una matrice:
\begin{proposizione}
  Sia $A$ una matrice di ordine $n$. Data una sua qualunque riga $\begin{pmatrix} a_{i1} & a_{i2} & \dots & a_{in} \end{pmatrix}$ o una qualungue colonna $\begin{pmatrix} a_{1j} \\ a_{2j} \\ \vdots \\ a_{nj} \end{pmatrix}$, si ha
  \begin{eqnarray}
    \det(A)=a_{i1}C_{i1}+a_{i2}C_{i2}+\dots+a_{in}C_{in}\\
    \det(A)=a_{1j}C_{1j}+a_{2j}C_{2j}+\dots+a_{nj}C_{nj}
  \end{eqnarray}
  La proposizione, che non dimostriamo, afferma quindi che il determinante di una matrice si può calcolare scegliendo una riga o una colonna, moltiplicando ogni elemento della riga o della colonna per il suo cofattore e sommando il tutto. La (3.9) e la (3.10) si dicono rispettivamente \textit{formula (o sviluppo) di Laplace secondo la i-esima riga e la la formula (o sviluppo) di Laplace secondo la j-esima colonna.}\\
  L'osservazione importante da fare sulla formula di Laplace è che essa esprime il determinante di una matrice di ordine $n$ in funzione dei suoi cofattori, che sono, a parte il fattore $(-1)^{i+j}$, determinanti di matrici di ordine $n-1$: a loro volta, potremo poi calcolare ciascuno di questi cofattori riutilizzando la formula di Laplace, con la quale ci ridurremo a calcolare determinanti di matrici di oridne $n - 2$, e così viam fino a che non arriveremo a matrici di ordine 2 per le quali la formula del determinante è particolarmente semplice da ricordare. Vediamo un esempio.
\end{proposizione}
\begin{esempio}
  Sia $A=\begin{pmatrix} 1 & 1 & 0 & 0 \\ 0 & 1 & -1 & 1 \\ 1 & 0 & 0 & 1 \\ 1 & 2 & 1 & 0\end{pmatrix}$. Scegliamo di sviluppare il determinante secondo Laplace rispetto alla terza riga, usando cioè la formula (3.9) nel caso $i = 3$:
  \begin{equation*}
    \det (A)=a_{31}C_{31}+a_{32}C_{32}+a_{33}C_{33}+a_{34}C_{34}.
  \end{equation*}
  Per definizione di cofattori e tenendo conto che $a_{32}=a_{33}=0$ (e quindi non abbiamo bisogno di calcolare i corrispondenti cofattori) si ha
  \begin{equation}
	\det(A)=1*(-1)^{3+1}\det\begin{pmatrix} 1 & 0 & 0 \\ 1 & -1 & 1 \\ 2 & 1 & 0\end{pmatrix} + 1*(-1)^{3+4}\det\begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & -1 \\ 1 & 2 & 1\end{pmatrix}
  \end{equation}
  Ora, cisascuno dei due determinanti di ordine 3 che compaiono in questa uguaglianza può essere calcolato usando di nuovo la formula data. Ad esempio, se per il primo determinante scegliamo la terza colonna (quindi dobbiamo usare $\det (A)=a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13}$) tenendo conto che $a_{13}=0$, si ha
  \begin{equation*}
	\det\begin{pmatrix} 1 & 0 & 0 \\ 1 & -1 & 1 \\ 2 & 1 & 0\end{pmatrix}=1*(-1)^{2+3}\det\begin{pmatrix} 1 & 0 \\ 2 & 1 \end{pmatrix}=-1.
  \end{equation*}
  dove il determinante di ordine 2 è stato calcolato usando la formula $\det(A)=a_{11}a_{22}-a_{12}a_{21}$.
  Analohgalmente, se per l'altro determinante scegliamo la prima riga (quindi dobbiamo usare
  $\det (A)=a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13}$) tenendo conto che $a_{13}=0$, si ha
  \begin{equation*}
    \det\begin{pmatrix} 1 & 1 & 0 \\ 0 & 1 & -1 \\ 1 & 2 & 1\end{pmatrix}=1*(-1)^{1+1}\det\begin{pmatrix} 1 & -1 \\ 2 & 1 \end{pmatrix} + 1 * (-1)^{3+1}\det\begin{pmatrix} 0 & -1 \\ 1 & 1\end{pmatrix} =3-1=2.
  \end{equation*}
  Sostituendo questi risultati nella (3.11), si trova allora
  \begin{equation*}
    \det(A)=1*(-1)^{3+1}(-1)+1*(-1)^{3+4}2=-3.
  \end{equation*}
  Nell'enunciato del Proposizione \ref{Laplace}, è sottointeso il fatto che il risultato dello sviluppo ci darà sempre il determinante di $A$ indipendentemente dalla riga o colonna scelta: questo è molto importante nella pratica in quanto ci consente, come visto anche nell-esempio, di scegliere se possibile righe o colonne in cui alcune in cui alcune entrate siano nulle, così da non dover calcolare i corrispondente cofattori. 
\end{esempio}
\begin{osservazione}
  Notiamo che l'espressione (3.7), che abbiamo usato per mostrare geometricamente che il determinante di una matrice di ordine 3 si annulla se e solo se le sue righe sono dipendenti, non era altro che lo sviluppo di Laplace di tale determinante rispetto alla prima riga: infatti, ricordando la derinizione di determinante di una matrice di ordine 2, si trava
  \begin{equation*}
	\begin{matrix}
          a_{11}(a_{22}a_{33}-a_{23}a_{32})-a_{12}(a_{21}a_{33}-a_{23}a_{31})+a_{12}(a_{21}a_{32}-a_{22}a_{31})=\\
          a_{11}(-1)^{1+1}\det\begin{pmatrix} a_{22} & a_{23} \\ a_{32} & a_{33}\end{pmatrix}+a_{12}(-1)^{1+2}\det\begin{pmatrix} a_{21} & a_{23} \\ a_{31} & a_{33}\end{pmatrix}+a_{13}(-1)^{1+3}\det\begin{pmatrix} a_{21} & a_{22} \\ a_{32} & a_{32}\end{pmatrix}=\\a_{11}C_{11}+a_{12}C_{12}+a_{13}C_{13}
        \end{matrix}
  \end{equation*} 
\end{osservazione}
\section{Proprietà del determionante}
In questo paragrafo dimostreremo finalmente che il determinante di una matrice $A$ si annulla \underline{se e solo se} le sue righe sono dipendenti. A questo scopo, iniziamo con l'enunciazione e dimostrazione tre importanti proprietà del determinante:
\begin{enumerate}
\item se $A^\prime$ si ottiene da $A$ moltipricando una riga di $A$ per $c\in \mathds{K}$, allora $\det(A^\prime)=c\det(A)$\\
  Ad esempio, se $A=\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ e $A^\prime =\begin{pmatrix} 5 & 10 \\ 3 &4 \end{pmatrix}$ è la matrice ottenuta da $A$ moltiplicando la prima riga per $c=5$, ho $\det(A^\prime)=5*4-10*3=5\det (A)$.\\
  In altre parole, se una riga di una matrice viene moltiplicata per una scala $c$, quando calcoliamo il determinante possiamo ``portare fuori'' questo scalare.\\
  In base alle definizione ({\it <++>}) di determinantem, il determinante della matrice $A^\prime$ in cui abbiamo moltiplicato tutti gli elementi della riga \textit{i-esima} per $c$ è $\det (A^\prime)=\sigma s(p)a_{1p(1)}\dots(ca_{ip(1)})\dots a_{np(n)}$: essendo il fattore $c$ comune a tutti gli addendi della sommatoria, possiamo metterlo in evidenze in eveidenza alla somma e scivere $\det(A^\prime) = c \sigma s(p)a_{ip(n)}a_{1np(n)}$, ovvero $\det(A^\prime)=c\det(A)$, come volevamo.
\item supponiamo di sommare a una riga $R_i$ di una matrice $A$ una \textit{n-upla} $v=(v_{i1},v_{i2},\dots,v_{in})$. Allora, il determinante della nuova matrice $A^\prime$ così ottenuta vale
  \begin{equation}
    \det(A^{\prime})=\det(A)+\det\begin{pmatrix} R_1 \\ \dots \\ v\\ \dots \\ R_n \end{pmatrix}
  \end{equation}
  dove stiamo denotando con $\begin{pmatrix} R_1 \\ \dots \\ v\\ \dots \\ R_n \end{pmatrix}$ la matrice ottenuta da $A$ sostituendo $v$ al posto della riga $R_i$. Possiamo anche scrivere
  \begin{equation}
	\det\begin{pmatrix} R_1 \\ \dots \\ R_i+v\\ \dots \\ R_n \end{pmatrix}=\det\begin{pmatrix} R_1 \\ \dots \\ R_i\\ \dots \\ R_n \end{pmatrix}+\det \begin{pmatrix} R_1 \\ \dots \\ v\\ \dots \\ R_n \end{pmatrix}
  \end{equation}
  Ad esempio, consideriamo la matrice $A=\begin{pmatrix} 1 & 2 \\ 2 & 1 \end{pmatrix}$ e sommiamo
  alla sua seconda riga (2, 1) la coppia (1, 3), ottenendo  $A=\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$. Allora si ha proprio
  \begin{equation*}
	\det(A^\prime)=\det\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}=\det(A)+\begin{pmatrix} 1 & 2 \\ 1 & 3 \end{pmatrix}
  \end{equation*}
  ovvero $-2=-3+1$.
		\begin{proof}
			Se $R_i=\begin{pmatrix}a_{i1} & a_{i2} & \dots & a_{in}\end{pmatrix}$
			allora la riga i-esima della matrice\\
			$A^\prime$ è $R_i+v+\begin{pmatrix}a_{i1}+v_{i1} & a_{i2}+v_{i2} & 
			\dots & a_{in}+v_{in}\end{pmatrix}$
			In base alla definizione (<++>) di determinante, si ha
			$\det(A^\prime)=\sum s(p)a_{ip(1)}\dots (a_{ip(i)}+v_{ip(i)})\dots a
			_{np(n)}=\sum s(p)a_{ip(1)}\dots a_{ip(i)}\dots a_{np(n)}+\sum s(p)a
			_{ip(1)}\dots v_{ip(i)}\dots a_{np(n)}$ ovvero 
			$\det(A)+\det\begin{pmatrix}R_1\\ \dots\\ v\\ \dots\\ 
			R_n\end{pmatrix}$, come volevamo.
		\end{proof}
    \item se $A^\prime$ si ottiene da $A$ scambiando tra loro due righe allora $\det(A^\prime)=-\det(A)$\\
		Ad esempio, se $A=\begin{pmatrix} 1 & 2 \\ 3 & 4 \end{pmatrix}$ e
			  $A^{\prime}=\begin{pmatrix} 4 & 3 \\ 1 & 2 \end{pmatrix}$ è la
			  matrice ottenuta da $A$ scambiando tra loro la prima e la seconda
			  riga, ho $\det(A)=1*4-2*3=-2$ e $\det(A^\prime)=3*2-4*1=+2$.
		\begin{proof}
			Sia $A^\prime$ la matrice ottenuta da $A$ scambiando la riga $R_i$
			con la $R_j$: quindi per ogni $k$ si ha $a^\prime_{ik}=a_{jk}$ e
			$a^\prime_{jk}=a_{ik}$, mentre tutte le altre sono uguali a quelle
			di $A$.\\
			Quindi, in base alla (3.1), si ha
			\begin{equation}
				\det(A^\prime) = \sum_{p\in Sn} s(p) a^\prime_{1p(1)}\dots
				a^\prime_{ip(i)}\dots a^\prime_{jp(j)}\dots a^\prime_{np(n)} =
				\sum_{p\in Sn} s(p) a^\prime_{1p(1)}\dots
				a^\prime_{jp(j)}\dots a^\prime_{ip(i)}\dots a^\prime_{np(n)}.
			\end{equation}
			(in pratica, l'unica differenza tra la (3.14) e l'espressione del
			determinante di $A$ è che in ogni addendo le entrate ``centrali''
			habbi gli indici di riga $i$ e $j$ scambiati tra logo).\\
			Ora, allo scopo di dimostrare il risultato, riscriviamo la (3.14)
			come segue: consideriamo la trasposizione $\mathcal{T}\in S_n$ che
			scambia $i$ con $j$ e lascia invariati tutti gli altri elementi:
			quindi $j=\mathcal{T}(i), \text{ } i=\mathcal{T}(j)$, e per tutti
			gli altri indici $k =\mathcal{T}(k)$.\\
			Allora possiamo scrivere
			\begin{equation}
				\det(A^\prime) = \sum_{p\in Sn} s(p) a_{1\mathcal{T}(1)}\dots
				a^\prime_{jp\mathcal{T}(j)}\dots a^\prime_{ip\mathcal{T}(i)}
				\dots a^\prime_{np\mathcal{T}(n)} 
			\end{equation}
			Ora, per un qualunque indice $k$, si ha che $p(\mathcal{T}(k))$ è
			il valore associato a $k$ dalla permutazione $p \mathcal{T}$ che si
			ottiene componendo $p$ con $\mathcal{T}$ (ovvero applicando, da
			destra, prima $\mathcal{T}$ e poi $p$). Tale permutazione è tale
			dispari, e di un numero pari di trasposizione (una in più di $p$),
			e analogamente se dispari di trasposizione, e allora
			$p\mathcal{T}$, sarà composta da un numero dispari di trasposizione
			(una in più di $p$), e analogamente se $p$ è dispari vuol dire che
			si scrive come composizione di un numero di strasposizioni.
			Riassumendo, la (3.15), scambiando anche di ordine i due fattori
			centrali $a_{j p\mathcal{T}(j)}$ e $a_{ip\mathcal{T}(i)}$ in ogni
			addendo (il prodotto gode della proprietà commutativa) può essere
			scritta nel modo seguente
			\begin{equation}
				\det(A^\prime) = -\sum_{p\in S_n} s(p\mathcal{T})
				a_{1p\mathcal{T}(1)} \dots a_{i p\mathcal{T}(i)} \dots a_{j
				p\mathcal{T}(j)} \dots a_{n p\mathcal{T}(n)}
			\end{equation}
			Ora, quest'ultima espressione, con il segno meno davanti, è uguale
			a quella del determinante di $A$ a parte che in essa compare
			$p\mathcal{T}$ invece che $p$. Ma in realtà questo non cambia
			nulla, in quanto facciamo variare $p$ tra tutti le permutazioni
			$p_1,p_2,\dots,p_{n!}$ di $n$ elementi, $p_1\mathcal{T},
			p_2\mathcal{T},\dots,p_{n!}\mathcal{T}$ sono ancora {\em tutte} le
			permutazioni: per non esserlo, dovrebbe infatti succedere che
                        $p_i\mathcal{T}=p_j\mathcal{T}$ allora
                        $p_i\mathcal{T}\mathcal{T}=p_j\mathcal{T}\mathcal{T}$, ovvero, essendo
                        $\mathcal{T}\mathcal{T}=id,p_i=p_j$.\\
                        Quindi, a parte il segno, la ({\bf 3.16}) contiene esattamente un
                        addendo per ogni permutazione, e otteniamo il risultato voluto.
		\end{proof}
\end{enumerate}
\clearpage              
La proprietà (3) ha l'importante conseguenza che se una matrice ha due {\it righe uguali allora
  il suo determinante è nullo.}\\
Infatti, in base alla (3) se $A^\prime$ è ottenuta da $A$ scambiando tra loro due righe allora
$det(A^\prime)=-det(A)$, ma se le righe scambiate sono proprio le due righe uguali allora
$A^\prime=A$, da cui concludiamo $det(A)=-det(A)$, ovvero $det(A)=0$. Combinando questo fatto
con le proprietà (2) e (1) possiamo capire come si comporta il determinante quando eseguiamo
operazioni elementari del terzo tipo: infatti, supponiamo che $A^\prime$ sia ottenuta da $A$
sommanda alla sua {\it i-esima} riga $R$, la {\it j-esima} riga $R_j$ moltiplicata per $c$,
ovvero $A^\prime =\begin{pmatrix}
R_1\\
\dots\\
R_i +cR_j\\
\dots\\
R_j\\
\dots\\
R_n
\end{pmatrix}
$.\\
Ma allora, per la proprietà (2) e la proprietà (1) ho
\begin{equation}
  \det(A^\prime)=\det\begin{pmatrix}
                     R_1\\
                     \dots\\
                     R_i +cR_j\\
                     \dots\\
                     R_j\\
                     \dots\\
                     R_n
                   \end{pmatrix}=\det
                   \begin{pmatrix}
                     R_1\\
                     \dots\\
                     R_i\\
                     \dots\\
                     R_j\\
                     \dots\\
                     R_n
                   \end{pmatrix}+\det
                   \begin{pmatrix}
                     R_1\\
                     \dots\\
                     cR_j\\
                     \dots\\
                     R_j\\
                     \dots\\
                     R_n
                   \end{pmatrix}=\det
                   \begin{pmatrix}
                     R_1\\
                     \dots\\
                     R_i\\
                     \dots\\
                     R_j\\
                     \dots\\
                     R_n
                   \end{pmatrix}+c\det
                   \begin{pmatrix}
                     R_1\\
                     \dots\\
                     R_j\\
                     \dots\\
                     R_j\\
                     \dots\\
                     R_n
                   \end{pmatrix}
\end{equation}
Ma mentre i primo addendo dopo l'ultima uguaglianza è il determinante della matrice $A$, il
secondo addendo è nullo, in quanto determinante di una matrice con le righe uguali (compare
due volte $R_j$). Quindi la (3.17) si legge come
\begin{equation*}
  \det(A^\prime)=\det(A)+0=\det(A).
\end{equation*}
Ora siamo finalmente pronti a dimostrare il
\begin{teorema}
  Una matrice $A$ quadrata di ordine $n$ ha rango $n$ (ovvero ha le righe indipendenti) se e
  solo se $det (A)\neq 0$.
  \begin{osservazione}
    per sapere se $n$ vettori di $\mathds{R}^\prime$ sono linearmente indipendenti oppure
    dipendenti, si dispongono le loro coordinate in una matrice e si calcola il determinante.
    \begin{itemize}
    \item Se $\det \neq 0$, allora i vettori sono lineari indipendenti.
    \item Se $\det = 0$, allora i vettori sono lineari dipendenti.
    \end{itemize}
  \end{osservazione}
  \begin{proof}
    Supponiamo che $A$ abbia rango massimo $n$. Allora, come sappiamo, nella matrice $A^\prime$ si
    ottiene da A mediante operazioni elementari, e come abbiamo visto sopra le operazioni
    elementari o non modificano il determinante ({\it terzo tipo}) o lo cambiano di segno (prima
    tipo) o lo moltiplicano per un coefficiente non nullo (secondo tipo), si avrà
    $\det(A^\prime)=\pm c_1\dots c_m\det (A)$, con $c_1,\dots,c_m\neq 0$, e per dimostrare che
    $\det(A^\prime)\neq 0$.\\
    Poiché stiamo dicendo che matrice quadrata a gradini senza righe nulle, $A^\prime$ è della
    forma
    \begin{eqnarray*}
      A^\prime=\begin{pmatrix}
                 a^\prime_{11} & a^\prime_{12} & \dots &a^\prime_{1n-1}& a^\prime_{1n}\\
                 0           & a^\prime_{22} & \dots &a^\prime_{2n-1}& a^\prime_{2n}\\
                               && \vdots\\
                 0 & 0 & \dots &a^\prime_{n-1n-1}& a^\prime_{n-1n}\\
                 0 & 0 & \dots & 0 & a^\prime_{nn}
               \end{pmatrix} & \begin{matrix}
                                 \text{Triangolare superiore con}\\
                                 \text{elementi sulla diagonale tutti}\\
                                 \text{diversi da zero}
                                 \end{matrix}
    \end{eqnarray*}
    con $a^\prime_{11},a^\prime_{12},\dots,a^\prime_{n-1n-1}, a^\prime_{nn}$ tutti divensi da zero.
    Calcoliamo il determinante di $A^\prime$ sviluppando secondo Laplace rispetto alla prima
    colonna: essendo $a^\prime_{11}$ l'unico elemento non nullo, si avrà
    \begin{equation*}
      \det(A^\prime)=a^\prime_{11}(-1)^{1+1}\det\begin{pmatrix}
                                                  a^\prime_{22} & \dots &a^\prime_{2n-1}
                                                  & a^\prime_{2n}\\
                               		          && \vdots\\
                  0 & \dots &a^\prime_{n-1n-1}& a^\prime_{n-1n}\\
                                                  0 & \dots & 0 & a^\prime_{nn}
                                                \end{pmatrix}
    \end{equation*}
    \clearpage
    Se sviluppiamo ora il cofattore di nuovo secondo Laplace rispetto alla sua prima colonna,
    che ha come unico elemento non nullo $a_{22}^\prime$, otteniamo
    \begin{equation*}
      \det(A^\prime)=a^\prime_{11}(-1)^{1+1}\det\begin{pmatrix}
                                                  a^\prime_{33} & \dots &a^\prime_{3n-1}
                                                  & a^\prime_{2n}\\
                               		          && \vdots\\
                  0 & \dots &a^\prime_{n-1n-1}& a^\prime_{n-1n}\\
                                                  0 & \dots & 0 & a^\prime_{nn}
                                                \end{pmatrix}
    \end{equation*}
    e continuando a sviluppare sempre rispetto alla prima colonna otteniamo $\det (A^\prime)=
    a^\prime_{11}a_{12}^\prime\dots a_{nn}^\prime$. Essendo come abbiamo detto qgli elementi
    $a^\prime_{11},a_{12}^\prime,\dots, a_{nn}^\prime$ tutti non nulli, si ha, come volevamo,
    $\det(A^\prime)\neq 0$: la prima implicazione è dimostrata.
    \begin{osservazione}
      il $\det$ diuna matrice triangolare è il prodotto degli elementi sulla diagonale
    \end{osservazione}
    Per quello che riguarda l'implicazione inversa se $\det (A)\neq 0$ allora $A$ ha rango $n$,
    essa si dimostra facilmente per assurdo osservando che se $A$ non avesse rango $n$, allora
    la matrice ridotta $A^\prime$ avrebbe una riga nulla, e sviluppando il suo determinante
    secondo Laplace proprio rispetto a quella riga otterremmo $\det(A^\prime) = 0$ (in quanto ogni
    sviluppo andrebbe moltiplicato per zero). ma poiché come abbiamo osservato nel dimostrare la
    prima implicazione il determinante della matrice ridotta è nullo se e solo se lo è quello
    della matrice di partenza, allora avremmo anche $\det(A)=0$, contro l'ipotesi.
  \end{proof}
  \begin{osservazione}
    Nell'osservazione <++> abbiamo visto un modo per passare da equazioni parametriche di un
    piano alla sua equazione cartesiana mediante eliminazione dei parametri $t$ e $s$ per
    sostituzione. Vediamo ora che grazie al determinante esiste un procedimento più elegante:
    le equazioni parametriche (\ref{eqpar}) possono essere riscritte nel seguente modo:
    \begin{equation*}
      \begin{pmatrix}
        x\\
        y\\
        z
      \end{pmatrix}
      =\begin{pmatrix}
         x_0\\
         y_0\\
         z_0
       \end{pmatrix}
       +t\begin{pmatrix}
           l\\
           m\\
           n
         \end{pmatrix}
         +s\begin{pmatrix}
            l^\prime\\
            m^\prime\\
            n^\prime
          \end{pmatrix}
    \end{equation*}
    ovvero
    \begin{equation*}
      \begin{pmatrix}
        x-x_0\\
        y-y_0\\
        z-z_0
      \end{pmatrix}=t\begin{pmatrix}
                       l\\
                       m\\
                       n
                     \end{pmatrix} +s
                     \begin{pmatrix}
                       l^\prime\\
                       m^\prime\\
                       n^\prime
                     \end{pmatrix}.
    \end{equation*}
    Quest'ultima ugualianza ci dice che il vettore $(x-x_0,y-y_0,z-z_0)$ si scrive come
    combinazione lineare di $(l,m,n)$ e $(l^\prime,m^\prime,n^\prime)$, e quindi che la matrice
    \begin{equation*}
      \begin{pmatrix}
        x-x_0 & l &l^\prime\\
        y-y_0 & m &m^\prime\\
        z-z_0 & n & n^\prime
      \end{pmatrix}
    \end{equation*}
    che ha tale vettore come colonne ha rango 2. Questo, per la proprietà appena dimostrata che
    afferma che una matrice quadrata di ordine n ha rango minore di n se e solo se il suo
    determinate è zero, equivale a dire che
    \begin{equation*}
      \det\begin{pmatrix}
        x-x_0 & l &l^\prime\\
        y-y_0 & m &m^\prime\\
        z-z_0 & n & n^\prime
      \end{pmatrix}=0
    \end{equation*}
    Sviluppando questo determinante, si ottiene quindi un'equazione in $x,y,z$ che è appunto
    l'equazione cartesiana del piano.
    \begin{esempio}
      supponiamo che il piano abbia equazioni parametriche $
      \begin{cases}
        x=1+t+s\\
        y=2-t+2s\\
        z=3+t-s
      \end{cases}
      $.
    \end{esempio}
    Allora, la sua equazione cartesiana è data da
    \begin{equation*}
      \det
      \begin{pmatrix}
        x-1 & 1 &1\\
        y-2 &-1 & 2\\
        z-3 &1 & -1
      \end{pmatrix}
    \end{equation*}
    ovvero, sviuppando per sempio secondo Laplace rispetto alla prima colonna,
    \begin{equation*}
      (x-1)\det
      \begin{pmatrix}
        -1 & 2\\
        1 & -1
      \end{pmatrix} -(y-2) \det
      \begin{pmatrix}
        1 & 1 \\
        1 & -1
      \end{pmatrix}+(z-3)\det
      \begin{pmatrix}
        1 &1 \\
        1 &-1
      \end{pmatrix}=0
    \end{equation*}
    \clearpage
    cioè, svolgendo i calcoli,
    \begin{equation*}
      -(x-1)+2(y-2)+3(z-3)=-x+2y+3z-12=0
    \end{equation*}
    Nel caso in cui il piano sia quello passante per tre punti non allineati di coordinate
    $(x_0,y_0,z_0),(x_1,y_1,z_1),$\\$(x_2,y_2,z_2)$, abbiamo visto nella (<++>) che in tal caso basta
    prendere $(l,m,n)=(x_1-x_0,y_1-y_0,z_1-z_0)$ e $(l^\prime,m^\prime,n^\prime)=
    (x_2-x_0,y_2-y_0,z_2-z_0)$, e quindi la (<++>) diventa
    \begin{equation}
      \det
      \begin{pmatrix}
        x-x_0 &x_1-x_0 & x_2-x_0\\
        y-y_0 & y_1-y_0 & y_2-y_0\\
        z-z_0 & z_1-z_0 & z_2-z_0
      \end{pmatrix}=0
    \end{equation}
    che ci da un modo per scrivere subito l'equazione cartesiana del piano che passa per i tre
    punti.
  \end{osservazione}
  \begin{osservazione}
    Tutti le proprietà del determinante che abbiamo visto valide per operazioni sulle righe sono
    vere anche se effettuiamo le stesse operazione sulle colonne, ovvero:
    \begin{enumerate}
    \item se $A^\prime$ si ottiene da $A$ moltiplicando una colonna di $A$ per $c\in \mathds{K}$,
      allora $\det(A^\prime)=c\det(A)$
      \begin{esempio}
        se $A=
        \begin{pmatrix}
          1 & 2\\
          3 & 4
        \end{pmatrix}
        $ e $A^\prime=
        \begin{pmatrix}
          5 & 2\\
          15 & 4
        \end{pmatrix}
        $ è la matrice ottenuta da $A$ moltiplicando la prima colonna per $c = 5$, ho $\det(A)=-2$
        e $\det(A^\prime)=5\times4-10\times3=-10=5\det(A)$.
      \end{esempio}
    \item se a una colonna $C_i$ di una matrice $A$ sommiamo una $n$-upla data $v$ (scritta in
      colonna) si ha
      \begin{equation*}
        \det(A^\prime)=\det(A)+\det
        \begin{pmatrix}
          C_i & \dots & v & \dots & C_n
        \end{pmatrix}
      \end{equation*}
      \begin{esempio}
        se alla seconda colonna $C_2$ della matrice $
        A=\begin{pmatrix}
            1 & 2 \\
            3 & 1 
        \end{pmatrix}
        $ sommiamo la coppia $v=
        \begin{pmatrix}
          0 \\
          3
        \end{pmatrix}
        $, ottenendo così $C^\prime_2=C_2+v=
        \begin{pmatrix}
          2 \\
          1
        \end{pmatrix} +
        \begin{pmatrix}
          0 \\
          3
        \end{pmatrix} =
        \begin{pmatrix}
          2\\
          4
        \end{pmatrix}
        $, si ha
        \begin{equation*}
          \det
          \begin{pmatrix}
            1 & 2\\
            3 & 4
          \end{pmatrix} =
          \det\begin{pmatrix}
                1 & 2 \\
                3 & 4
          \end{pmatrix} + \det
          \begin{pmatrix}
            1 &0\\
            3 &3
          \end{pmatrix}
        \end{equation*}
        come può verificato calcolando i determinanti (si ottiene $-2=-5+3$).
      \end{esempio}
    \item se $A^\prime$ si ottiene da $A$ scambiando tra loro due colonne, allora $\det(A^\prime)
      =-\det(A)$
      \begin{esempio}
        se
        $
          A=\begin{pmatrix}
            1 & 2\\
            3 & 4
          \end{pmatrix}
        $ e
        $
          A^\prime=
          \begin{pmatrix}
            2 & 1\\
            4 & 3
          \end{pmatrix}$
        è la matrice ottenuta da $A$ scambiando tra loro la prima e la seconda colonna,
        ho $\det(A)=1*4-2*3=-2$ e $\det(A^\prime)=2*3 -1* 4=+2$.       
      \end{esempio}
      Omettiamo la dimostrazione di queste tre proprietà.
    \end{enumerate}
    Concludiamo questo capitolo mostrando come, benché non si possa calcolare il determinante di
    una matrice non quadrata, il determinante posso comunque essere usato per calcolare il rango
    di qualunque matrice.\\
    Più precisamente, data una matrice $A$, chiamiamo {\it sottomatrice di} $A$ una matrice che si ottiene da $A$
    scegliendo alcune righe e alcune colonne e cancellando le rimanenti. 
  \end{osservazione}
  \begin{esempio}
    se $A=
    \begin{pmatrix}
      0 & 1 & 1 &1 \\
      1 & 2 & 2 & 3 \\
      -1 & 0 &0 & -1
    \end{pmatrix}
    $, allora scegliendo la prima e la terza riga, e tra la quarta colonna\footnote{ovvero cancellando la seconda
    riga e le prime due colonne} si ottiene la sotto matrice $
  \begin{pmatrix}
    1 &1 \\
    0 & -1 
  \end{pmatrix}
  $
  \end{esempio}
\end{teorema}
 \begin{teorema}\label{3.9}
  Il rango di una matrice $A$ è k se e solo se esiste una sottomatrice di A di ordine k con determinante diverso
  da zero e tutte le sotto matrici quatrate di ordine $k+1$ hanno determinante nullo. 
\end{teorema}
\begin{esempio}
    la sottomatrice di ordine 1 con determinante\footnote{Una sottomatrice quadrata $A=(a)$
      di ordine 1, che contiene una sola entrata, ha come determinante $\det A=(a)$.} diverso da zero, per esempio
    $(a_{12})=(1)$, e tutte le sue sottomatrici quadrate di ordine 2, ovvero $
    \begin{pmatrix}
      0 & 1\\
      0 & 3
    \end{pmatrix}
    $,
    $
    \begin{pmatrix}
      0 &2 \\
      0 & 6
    \end{pmatrix}
    $, $
    \begin{pmatrix}
      1 &2\\
      3 & 6
    \end{pmatrix}
    $ hanno, come si vede immediatamente, il determinante uguale a zero: in base al teorema \textbf{3.9} la
    matrice ha rango 1, come in effetti già si vede notando che le sue due righe sono proporzionali.
\end{esempio}
In generale, per calcolare il rango di una matrice mediante il Teorema \textbf{3.9} basta iniziare a vedere se ci
sono sottomatrici di ordine 1 con determinante diverso da zero (ovvero se c'è un'entrata non nulla); se non ne
troviamo nessuna, concludiomo che il rango è zero; se ne troviamo almeno una, andiamo a vedere se ci sono
sottomatrice di ordine 3 e così via. Lo svantaggio di tale metodo è che può risultare molto laborioso calcolare
tutti i determinanti di tutte le sottomatrici di $A$, per i vari ordini. Tuttavia, esso può essere semplificato
nel modo sequente, che ci consente di controllare solo alcune sottomatrici e non tutte.\\
Più precisamente, data una sottomatrice quadrata $A^\prime$ di $A$ ottenuta scegliendo $k$ righe e $k$ colonne,
diciamo che una riga e una colonna a quelle già scelte per determinare $A^\prime$.
\begin{esempio}
  nella matrice $A=
  \begin{pmatrix}
    0 & 1 & 1 & 1 \\
    1 & 2 & 2 & 3 \\
    -1 & 0 & 0 & -1
  \end{pmatrix}
  $ qià vista sopra, la sottomatrice $A=
  \begin{pmatrix}
    1 & 1 \\
    0 & -1
  \end{pmatrix}
  $ è stata ottenuta scegliendo la prima e  la terza riga, e la quanta; aggiungendo ad esempio la seconda riga e
  la prima colonna ottengo $A^{\prime\prime}=
  \begin{pmatrix}
    0 &1 &1\\
    1 & 2& 2\\
    -1 & 0 & 0
  \end{pmatrix}
  $: quindi $A^{\prime\prime}$ si ottiene orlando $A^\prime$. La matrice $
  \begin{pmatrix}
    0 &1&1\\
    1&2&2\\
    -1&0&0
  \end{pmatrix}
  $, che corrisponde a scegliere prima, seconda e la terza riga, e prima, seconda e terza colonna, non è invece
  ottenuta orlando $A^\prime$.
\end{esempio}
Ora, il teorema precendente può essere migliorato come segue.
\begin{teorema}
  (dei minimi orlati) Il rango di una matrice $A$ è $k$ se e solo se esiste una sottomatrice
  quadrata $A^\prime$ di $A$ di ordine $k$ con determinante diverso da zero e tutte le
  sottomatrici quadrate di ordine $k+1$ ottenute orlando $A^\prime$ hanno determinante nullo.
\end{teorema}
Quindi per calcolare il rango di $A$ possiamo procedere come seque: si inizia a vedere se ci sono
sottomatrici di ordine 2 con determinante diverso da zero; se non ne traviamo nessuna,
concludiamo che il rango è 1; se ne troviamo almeno una, diciamo $A^\prime$, andiamo a vedere se
tra le sottomatrici di ordine 3 ottenute orlando $A^\prime$ (non è quindi necessario vederle
tutte) c'è né sono con era 2, se ne troviamo almeno una, diciamo $A^{\prime\prime}$ andiamo
a vedere quelle di ordine 4 che si ottengono orlando $A^{\prime\prime}$ e così via.
\begin{esempio}
  Calcoliamo il gango della matrice $A=
  \begin{pmatrix}
    0 &1 &1&1\\
    1 &2&2 &3\\
    -1 & 0 & 0& -1
  \end{pmatrix}
  $ usando il Teorema 3.10: prendiamo la sottomatrice $A^\prime=
  \begin{pmatrix}
    1 & 1\\
    2 & 3 
  \end{pmatrix}
  $ di ordine 2 ottenuta scegliendo prima e seconda riga, e taerza e quarda colonna: essa ha
  determinante uguale a $1 \neq 0$, quindi il rango di $A$ è almeno 2; per vedere se è 2 o 3,
  consideriamo le matrice di ordine 3 ottenute orlando $A^\prime$, ovvero $A^{\prime\prime}=
  \begin{pmatrix}
    1 &1 &1 \\
    2 & 2 &3 \\
    0 &0 &-1
  \end{pmatrix}
  $ (ottenuta aggiungendo la teza riga e la seconda colonna) e $A^{\prime\prime\prime}=
  \begin{pmatrix}
    0&1 &1 \\
    1 & 2 & 3 \\
    -1 & 0 & -1
  \end{pmatrix}
  $ (ottenuta aggiungendo la teza riga e la prima colonna).\\
  Ora, da un calcolo diretto usando la sviluppo di Laplace (\ref{Laplace}) si vede che $\det (A^{\prime\prime})=0$
  e $\det(A^{\prime\prime\prime})=0$, quindi concludiamo che il rango della matrice è 2.
\end{esempio}
   
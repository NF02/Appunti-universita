
\chapter{Applicazioni lineari e prodotti di matrici}
\label{chap:appLinEprodotdimatrix}

\section{Applicazioni lineari: definizione ed esempi}
\label{sec:applindefes}

In questo paragrafo si parlerà di funzioni tra spazi vettoriali.
Ricordando che una funzione $f:X\to Y$ tra due insiemi $X$
(detto \textit{dominio}) e $Y$ (detto \textit{codominio}) è una legge che
associa a ogni $x\in X$ un ben preciso elemento di $Y$, detto
\textit{immagine di $x$} e denotato $f(x)$.
Tipicamente vengono trattate le funzioni $f:V\to W$ in cui dominio e
codominio sono, corrispettivamente, $V$ e $W$, spazi vettoriali su un
certo campo $\mathds{K}$, e in particolare si andrà a studiare quello che
soddisfa la seguente
\begin{defi}
  \label{defi:applindefes}
  Una funzione $f:V\to W$ tra spazi vettoriali si dice \textit{funzione
    lineare} (o \textit{applicazione lineare}) se verifica le due seguenti
  proprietà:
  \begin{equation}
    \label{eq:applindefes1}
    f(v+v^\prime)=f(v)+f(v^\prime) \text{ per ogni } v,v^\prime \in V 
  \end{equation}
  \begin{equation}
    \label{eq:applindefes2}
    f(cv)=cf(v) \text{ per ogni $v\in V$ e ogni scalare } c\in\mathds{K}
  \end{equation}
  Limitarsi, alla funzione lineare può essere molto restrittivo: ad
  esempio, si può vedere che se\footnote{Sapendo che $\mathds{R}^n$ è uno
    spazio vettoriale di dimensione $n$, in particolare per $n=1$ si
    ottiene $\mathds{R}^1=\mathds{R}$ (che risulta quindi uno spazio
    vettoriale di dimensione unitaria o $dim = 1$)} $V=W=\mathds{R}$, le
  uniche funzioni lineari $f:\mathds{R} \to \mathds{R}$ sono quelle del
  tipo $f(x)=ax$, con $a\in \mathds{R}$ fissato. Tuttavia, bisogna notare
  che tra le applicazioni lineari vi sono funzioni di grande importanzia
  e utilità in geometria e nelle sue applicazioni:
\end{defi}
\begin{es}
  \label{es:applindefes1}
  Dato lo spazio $V_O^2$ dei vettori geometrici applicati nel piano,
  considerando la funzione $f:V_O^2\to V_O^2$ che associa a ongi vettore
  $\vec{OP}$ il vettore che si ottiene ruotado $\vec{OP}$ di un angolo
  $\theta$ fissato in senso antiorario attorno all'origine $O$, come
  nella figura seguente
  \begin{figure}[ht!]
    \centering
    \resizebox{4cm}{!}{\input{img/appLinEprodotdimtx1.tikz}}
    \caption{Funzione $f:V_O^2\to V_O^2$ associata ai vettori $\vec{OP}$ e $\vec{OP}$}
    \label{fig:applindefes1}
  \end{figure}
  
  Come si vede nella figura seguente, dati due vettori $\vec{OP}$ e
  $\vec{OP}^\prime$, sommarli e poi ruotare il vettore risultante oppure
  prima ruotarli e poi sommare i vettori ruotati è equivalente, ovvero
  \clearpage
  \begin{figure}[ht!]
    \centering
    \resizebox{7cm}{!}{\input{./img/appLinEprodotdimtx2.tikz}}
    \caption{Cosa accade se si ruota e sommano i vettori $\vec{OP}$ e $\vec{OP}^\prime$}
    \label{fig:applindefes2}
  \end{figure}
  Quindi vale la formula, già acclarata
  \begin{equation}
    \label{eq:applindefes3}
    f(\vec{OP})+f(\vec{OP}^\prime)=f(\vec{OP}+\vec{OP}^\prime)
  \end{equation}
  che dice che questa funzione soddisfa la proprietà
  (\ref{eq:applindefes1}). Analoramente, dato un vettore $\vec{OP}$ e un
  numero reale $c$, moltiplicare il vettore per $c$ e poi ruotarlo oppure
  prima ruotarlo e poi moltiplicarlo per $c$ è equivalente:
  \begin{figure}[ht!]
    \centering
    \resizebox{5cm}{!}{\input{./img/appLinEprodotdimtx3.tikz}}
    \caption{Moltiplicazione tra il vettore $\vec{OP}$ e un numero reale $c$}
    \label{fig:applindefes3}
  \end{figure}
  
  Quindi si ha, l'equivalenza:
  \begin{equation}
    \label{eq:applindefes4}
    f(c\vec{OP})=cf(\vec{OP})
  \end{equation}
  che afferma, in modo abbastanza esplicito che, questa funzione anche
  la proprietà (\ref{eq:applindefes2}) della Definizione
  \ref{defi:applindefes}. E si conclude quindi, che le rotazioni attorno
  a $O$ sono applicazioni lineari dallo spazio vettoriale $V_O^2$ in se
  stesso.\\
  È possibile raggiungere la stessa conclusione anche per altre importanti
  trasformazioni geometriche; ad esempio, si consideri la riflessione
  rispetto a una retta $r$ passante per $O$, che manda ogni vettore
  $\vec{OP}\in V_O^2$ nel vettore sommetrico rispetto alla retta, come
  da figura
  \begin{figure}[ht!]
    \centering
    \resizebox{5cm}{!}{\input{./img/appLinEprodotdimtx4.tikz}}
    \caption{Metodo alternativo per dimostrare la funzione
      \ref{fig:applindefes1} mediante retta}
    \label{fig:applindefes4}
  \end{figure}

  Allora, Come già fatto per le rotazioni, si nota che, dati due vettori
  $\vec{OP}$ e $\vec{OP}^\prime$, sommarli e poi riflettere il vettore
  risultante oppure prima rifletterli e poi sommare i vettori riflessi è
  equivalente
  \clearpage
  \begin{figure}[ht!]
    \centering
    \resizebox{7cm}{!}{\input{./img/appLinEprodotdimtx5.tikz}}
    \caption{Metodo alternativo per dimostrare la funzione
      \ref{fig:applindefes2} mediante retta}
    \label{fig:applindefes5}
  \end{figure}
  e quindi anche in questo caso si otterà una formula identica alla
  \ref{eq:applindefes3}. Dato un vettore $\vec{OP}$ e un numero reale c,
  moltiplicare il vettore per $c$ e poi rifletterlo oppure prima
  rifletterlo e poi moltiplicarlo per $c$ è equivalente
  \begin{figure}[ht!]
    \centering
    \resizebox{7cm}{!}{\input{./img/appLinEprodotdimtx6.tikz}}
    \caption{Metodo alternativo per dimostrare la funzione \ref{fig:applindefes3} mediante retta}
    \label{fig:applindefes6}
  \end{figure}

  quindi, la formula è identica alla \ref{eq:applindefes4}, quindi
  è possibile concludere che anche la riflessione rispetto a una retta che
  passa per $O$, avendo le proprietà (\ref{eq:applindefes1}) e
  (\ref{eq:applindefes2}) richieste nella Definizione
  \ref{defi:applindefes}, è un'applicazione lineare $f:V_O^2\to V_O^2$.\\
  Come terzo esempio di applicazione lineare $V_O^2\to V_O^2$ bisogna
  citare la proiezione ortogonale, che proietta ortogonalmente i vettori
  su una retta fissata passante per $O$.
  \begin{figure}[ht!]
    \centering
    \resizebox{8cm}{!}{\input{./img/appLinEprodotdimtx7.tikz}}
    \caption{Retta con vettori proiettanti ortogonalmente per $O$}
    \label{fig:applindefes7}
  \end{figure}

  per la quale non è difficile vedere che valgono anche le proprietà
  (\ref{eq:applindefes1}) e (\ref{eq:applindefes2}).
  Analogamente a quanto visto per rotazioni, riflessioni e proiezioni nel
  piano, anche le corrispondenti trasformazioni $V_O^3\to V_O^3$ dello
  spazio tridimensionale $V_O^3$ sono applicazioni lineari; più
  precisamente, si può motrare che soddisfano le proprietà
  (\ref{eq:applindefes1}) e (\ref{eq:applindefes2}) della Definizione
  \ref{defi:applindefes} la rotazione di un angolo fissato $\theta$
  attorno a una retta data passante per $O$ (detta anche \emph{asse della
    rotazione}).
  \clearpage
  \begin{figure}[ht!]
    \centering
    \resizebox{3cm}{!}{\input{./img/appLinEprodotdimtx8.tikz}}
    \caption{La rotazione di un angolo fissato $\theta$ (asse della rotazione)}
    \label{fig:applindefes8}
  \end{figure}
  la riflessione rispetto a un piano passante per $O$
  \begin{figure}[ht!]
    \centering
    \resizebox{4cm}{!}{\input{./img/appLinEprodotdimtx9.tikz}}
    \caption{Riflessione rispetto a un piano passante per $O$}
    \label{fig:applindefes9}
  \end{figure}
  
  e la proiezione ortogonale su un piano passante per l'origine $O$:
  \begin{figure}[ht!]
    \centering
    \resizebox{5cm}{!}{\input{./img/appLinEprodotdimtx10.tikz}}
    \caption{Proiezione ortogonale su un piano passante per l'origine $O$}
    \label{fig:applindefes10}
  \end{figure}
\end{es}

\section{Matrice associata a un'applicazione lineare}
\label{sec:mtxAsaplin}

Una delle caratteristiche fondamentali di un'applicazione lineare
$f:V\to W$ è che, se gli spazi $V$ e $W$ hanno dimensione finita, allora
$f$ può essere rappresentata da una matrice.\\
I Dettagli: sia $f:V\to W$ un'applicazione lineare, e siano
$B_V=\{v_1,v_2,\dots,v_n\}$ e $B_W=\{w_1,w_2,\dots,w_m\}$ basi di $V$ e
$W$ rispettivamente. Allora, ogni vettore $v\to V$ può essere identificato
con un $n$-uple $(x_1,x_2,\dots,x_n)$, quella delle sue coordinate
rispetto alla base $B_V$, ovvero
\begin{equation}
  \label{eq:mtxAsaplin1}
  v=x_1v_1+x_2v_2+\cdots+x_nv_n),
\end{equation}
e analogamente ogni vettore $w\in W$ può essere identificato con una
$m$-upla $(y_1,y_2,\dots,y_m)$, quella delle sue coordinate rispetto alla
base $B_W$, ovvero
\begin{equation}
  \label{eq:mtxAsaplin2}
  w=y_1w_1+y_2w_2+\cdots+y_mw_m.
\end{equation}
Con queste identificazioni, la $f$ può essere pensata come una funzione
$\mathds{K}^n\to\mathds{m}$ che associa a ogni $n$-uple una $m$-upla.

L'obbiettivo a questo punto, è quello di esplicitare la funzione, a tale
scopo, sia $(x_1,\dots,x_n)$ la $n$-upla delle coordinate di un vettore
$v\in V$, ovvero come visto riportato prima, nella formula
(\ref{eq:mtxAsaplin1}). Allora la sua immagine $f(v)$ sarà
\begin{equation}
  \label{eq:mtxAsaplin3}
  f(v)=f(x_1v_1+\cdots+x_nv_n)=f(x_1v_1)+\cdots+f(x_nv_n)=x_1f(v_1)+\cdots+
  x_nf(v_n).
\end{equation}
Ora, ciascuno dei vettori $f(v_1),f(v_2),\dots,f(v_n)$ che compare nella
(\ref{eq:mtxAsaplin3}) appartiene al codominio $W$ della funzione, e
quindi potrà essere espresso come combinazione lineare dei vettori
$w_1,\dots,w_m$ della base $B_W$ fissata per $W$:
\begin{eqnarray}
  \label{eq:mtxAsaplin4}
  f(v_1)=a_{11}w_1+a_{21}w_2+\cdots+a_{m1}w_{m}
\end{eqnarray}
\begin{equation*}
  \vdots
\end{equation*}
\begin{equation}
  \label{eq:mtxAsaplin5}
  f(v_n)=a_{1n}w_1+a_{2n}w_2+\cdots+a_{nm}w_m
\end{equation}
Ora, sostituendo queste espressioni nella (\ref{eq:mtxAsaplin2}) si
ottiene
\begin{equation}
  \label{eq:mtxAsaplin6}
  \begin{matrix}
    f(v)=x_1(a_{11}w_1+a_{21}w_2+\cdots+a_{m1}w_m)+\cdots+x_n(a_{1n}w_1
    +a_{2n}w_2+\cdots+a_{mn}w_m)=\\
    =(a_{11}x_1+\cdots+a_{1n}x_n)w_1+\cdots+(a_{m1}x_1+\cdots+a_{mn}x_n)w_m
  \end{matrix}
\end{equation}
Questa uguaglianza sta dicendo che le coordinate del vettore $f(v)$
respetto alla base $B_W=\{w_1,w_2,\dots,w_m\}$ sono date da
$a_{11}x_1+\cdots+a_{1n}x_n,\dots,a_{m1}x_1+\cdots+a_{mn}x_n$ e quindi che,
tradotta in coordinate, la nostra applicazione lineare può essere
identificata con la funzione $\mathds{K}^n\to\mathds{K}^m$ che associa a
ogni $n$-upla $(x_1,\dots,x_n)$ la $m$-upla formata dai coefficienti che
appaiono nella (\ref{eq:mtxAsaplin6}), ovvero
\begin{equation}
  \label{eq:mtxAsaplin7}
  \begin{bmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_n
  \end{bmatrix}\to
  \begin{bmatrix}
    a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n\\
    a_{21}x_1+a_{21}x_2+\cdots+a_{2n}x_n\\
    \vdots\\
    a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n
  \end{bmatrix}
\end{equation}
I coefficienti che compaiono nella \ref{eq:mtxAsaplin7} formano una
matrice con $m$ righe e $n$ colonne
\begin{equation*}
  A=
  \begin{bmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    a_{21} & a_{22} & \cdots & a_{2n}\\
           & \vdots\\
    a_{m1} & a_{m2} & \cdots & a_{mn}
  \end{bmatrix}
\end{equation*}
la \emph{matrice associata all'applicazione lineare rispetto alle basi}
$B_V$ e $B_W$. In base alle
(\ref{eq:mtxAsaplin6}),\dots,(\ref{eq:mtxAsaplin7}) tale matrice può
essere definita come \emph{la matrice ha sulle colonne le coordinate dei
  vettori\\ $f(v_1),\dots,f(v_n)$}\footnote{Ovvero le immagini dei vettori
  della base $B_v$ fissato nel dominio} \emph{rispetto alla base $B_W$
  fissata nel codominio}. Si denota che $M_{B_w B_v}(f)$ la matrice
associata a un'applicazione lineare $f:V\to W$ rispetto alle basi $B_V$
e $B_W$. Se dominio e codominio dell'applicazione coincidono, ovvero ha
una funzione lineare $f:V\to V$ (tali applicazioni si dicono
\emph{endomorfismi}), allora è possibile fissare la stessa base $B_V$
sia nel dominio che nel codominio, e calcolare la matrice associata
$M_{B_w B_v} (f)$: in tal caso, per brevità la si denota semplicamente
$M_{B_v} (f)$. In generale, data $A$, si richiamerà la \emph{funzione
  determinante $A$ la funzione definita dalla \ref{eq:mtxAsaplin7}}, e
la si denota come $L_A$.

La matrice associata a un'applicazione lineare dà tutte le informazioni
necessarie sull'applicazione, e usando gli strumenti imparati nei capitoli
precedenti (rango, determinante) sarà possibile denotare molte proprietà
della funzione data. Ma prima di esplorare questo aspetto è giusto, fare un
esempio di quello finora esposto.
\clearpage
\begin{es}
  \label{es:mtxAsaplin1}
  Questo esempio sarà esposto per punti per un semplice fattore di
  comodità, quindi\dots
  \begin{enumerate}
  \item Sia $f:V_O^2\to V_O^2$ la rotazione attorno a $O$ di un angolo
    fissato $\theta$, come già esposto nei paragrafi precedenti.\\
    Per calcolare la matrice associata, bisogna fissare una base $B$
    formata da due vettori $v_1=\vec{OP}_1$ e $v_2=\vec{OP}_2$ della
    stessa lunghezza e perpendicolari tra loro, come da figura
    \begin{figure}[ht!]
      \centering
      \resizebox{8cm}{!}{\input{./img/mtxAsaplin1.tikz}}
      \caption{Base $B$ formata da due vettori $v_1$ e $v_2$ formati da $\vec{OP}$}
      \label{fig:mtxAsaplin1}
    \end{figure}
    e si determina la matrice $M_B(f)$.\\
    A questo scopo, come afferma la definizione di matrice associata, bisogna
    trovare le coordinate di $f(v_1)$ e $f(v_2)$ rispetto a $B$, ovvero esprimere
    $f(\vec{OP}_1)$ e $f(\vec{OP}_2)$ come combinazone lineare $x_1\vec{OP}_1+
    x_2\vec{OP}_2$ dei vettori della base $B$. Partendo con $f(\vec{OP}_1)$: come
    si vede dalla figura
    \begin{figure}[ht!]
      \centering
      \resizebox{8cm}{!}{\input{./img/mtxAsaplin2.tikz}}
      \caption{Sviluppo di $f(\vec{OP}_1)$}
      \label{fig:mtxAsaplin2}
    \end{figure}

    si ha quindi $f\left(\vec{OP}_1\right)=\vec{OP_1}=\vec{OA_1}+\vec{OB_1}$,
    essendo $A_1$ e $B_1$ le proiezioni ortogonali di $R_1$ sui vettori di
    base. Ora, chiaramente $\vec{OA}_1=x_1\vec{OP}_1$ e $\vec{OB_1}=x_2\vec{OP}_2$,
    dove $x_1$ è dato dal rapporto $\frac{\abs{\vec{OA}_1}}{\abs{\vec{OP}_1}}$ tra
    la lunghezza di $\vec{OA}_1$ e quella di $\vec{OP_2}$. Ma essendo la lunghezza
    di $\vec{OP_1}$ uguale alla lunghezza di $\vec{OR}_1=f(\vec{OP}_1)$, è possibile
    affermare che $x_1$ è uguale al rapporto tra la lunghezza del cateto $\vec{OA}_1$
    e quella dell'ipotenusa $\vec{OP}_1$ del triangolo rettangolo $OR_1A_1$, ovvero,
    $x_1=\cos\theta$.\\
    In modo analogo, poiché $\vec{OP}_2$ ha la stessa lunghezza di $\vec{OP_1}$ e
    quindi di $\vec{OR}_1$, ha la stessa lunghezza del segmento $A_1R_1$, si ha
    che
    \clearpage
    \begin{eqnarray*}
      x_2=\frac{\abs{\vec{OB}_1}}{\abs{\vec{OP_2}}}=\frac{\abs{A_1R_1}}{\abs{OR_1}},
    \end{eqnarray*}
    ovvero, $x=\sin\theta$. Riassumento,
    \begin{eqnarray}
      \label{eq:mtxAsaplin8}
      f(\vec{OP_1})=\vec{OR}_1=\vec{OA}_1+\vec{OB_1}=\cos{\theta}\vec{OP_1}+\sin\theta
      \vec{OP}_2
    \end{eqnarray}
    Per svolgere $f(\vec{OP_2})$, basterà fare l'analogo ragionamento
    \begin{figure}[ht!]
      \centering
      \resizebox{8cm}{!}{\input{./img/mtxAsaplin3.tikz}}
      \caption{Sviluppo di $f(\vec{OP}_2)$}
      \label{fig:mtxAsaplin3}
    \end{figure}

    si ha $f(\vec{OP_2})=\vec{OP}_2=\vec{OA_2}+\vec{OB_2}$. chiaramente,
    $\vec{OA_2}=-x_1\vec{OP}_1$, dove $x_1$ è dato dal rapporto $\frac{\abs{\vec{OA}_2}}
    {\abs{\vec{OP}_1}}$ tra la lunghezza di $\vec{OA}_2$ e quella di $\vec{OP}_1$ e
    $\vec{OB_2}=x_2\vec{OP_2}$, dove $x_2$ è dato dal rapporto $\frac{\abs{\vec{OB}_2}}
    {\abs{\vec{OP_2}}}$ tra la lunghezza di $\vec{OB}_2$ e quindi $\vec{OR_2}=f(\vec{OP_2})$,
    mentre, la lunghezza di $\vec{OA_2}$ è uguale alla lunghezza del segmento $R_2B_2$, è
    possibile affermare che $x_1$ è uguale al rapporto tra la lunghezza del cateto $R_2B_2$
    e quella dell'ipotenusa $OR_2$ del triangolo rettangolo $OR_2B_2$, ovvero, $x_2=\cos\theta$.
    
    Analogamente, poiché $\vec{OP}_2$ ha la stessa lunghezza di
    $\vec{OR}_2$, si ha che $x_2=\frac{\abs{\vec{OB}_2}}
    {\abs{\vec{OP}_2}}$, che è il rapporto tra il cateto e l'ipotenusa
    del triangola rettangolo $OB_2R_2$, ovvero, $x_2=\cos\theta$.

    Riassumendo,
    \begin{equation}
      \label{eq:mtxAsaplin9}
      f(\vec{OP_1})=\vec{OR}_2=\vec{OA_1}+\vec{OB}_1=\sin\theta
      \vec{OP}_1+\cos\theta\vec{OP}_2
    \end{equation}
    Quindi, (\ref{eq:mtxAsaplin8}) e (\ref{eq:mtxAsaplin9}) dicono
    che la matrice associata a $f$ rispotto a $B$ avrà sulla prima
    colonna $(\cos\theta, \sin\theta)$ e sulla seconda colonna
    $(-\sin \theta, \cos\theta)$, ovvero
    \begin{equation}
      \label{eq:mtxAsaplin10}
      M_B(f)=
      \begin{vmatrix}
        \cos\theta & -\sin\theta\\
        \sin\theta & \cos \theta
      \end{vmatrix}
    \end{equation}
    In base a quanto visto nella (\ref{eq:mtxAsaplin7}), si ottiene
    allora che la rotazione, in cordinate, si traduce in funzione di
    $f:\mathds{R}^2\to\mathds{R}^2$ data da
    \begin{equation}
      \label{eq:mtxAsaplin11}
      (x_1,x_2)\to (\cos\theta{}x_1-\sin\theta{}x_2,\sin\theta{}x_1
      +\cos\theta{}x_2)
    \end{equation}
    Ad esempio, scegliendo un angolo $\theta=\frac{\pi}{4}$ e si
    sostituisce in (\ref{eq:mtxAsaplin10}) e (\ref{eq:mtxAsaplin11}),
    considerando che $\cos\frac{\pi}{4}=\sin\frac{\pi}{4}=\frac{\sqrt{2}}{2}$,
    si ottengono corrispettivamente;
    \begin{eqnarray}
      \label{eq:mtxAsaplin12}
      M_B(f)=
      \begin{pmatrix}
        \frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2}\\
        \frac{\sqrt{2}}{2} & \frac{\sqrt{2}}{2}
      \end{pmatrix} & e & (x_1,x_2)\to
                          \begin{pmatrix}
                            \frac{\sqrt{2}}{2}x_1-\frac{\sqrt{2}}{2}x_2,
                            \frac{\sqrt{2}}{2}x_1+\frac{\sqrt{2}}{2}x_2
                          \end{pmatrix}
    \end{eqnarray}
    Per illustrare come questa semplice funzione $\mathds{R}^2\to \mathds{R}^2$
    rappresenti effettivamente la rotazione di $\frac{\pi}{4}$, è il caso di
    utilizzare un esempio, prenendo il vettore $v=v_1+v_2$, che come si vede
    dalla figura seguente, coincide con la diagonale del quadrato che ha $v_1$ e
    $v_2$ come lati:
    \clearpage
    \begin{figure}[ht!]
      \centering
      \resizebox{5cm}{!}{\input{./img/mtxAsaplin4.tikz}}
      \caption{Quadrato composto da $v_1$ e $v_2$}
      \label{fig:mtxAsaplin4}
    \end{figure}
    Tale vettore ha quindi come coordinate rispettoa $B$ la coppia $(x_1,x_2)=(1,1)$.
    In base alla (\ref{eq:mtxAsaplin12}), le coordinate di $f(v)$ rispetto a $B$ sono
    quindi date da
    \begin{equation*}
      \begin{pmatrix}
        \frac{\sqrt{2}}{2}\cdot 1 - \frac{\sqrt{2}}{2} \cdot 1,
        \frac{\sqrt{2}}{2} \cdot 1 + \frac{\sqrt{2}}{2}\cdot 1
      \end{pmatrix}
      =(0,\sqrt{2})
    \end{equation*}
    ovvero deve essere $f(v)=0v_1+\sqrt{2}v_2=\vec{2}v_2$.

    In effetti, tale risultato ottenuto analiticamente in coordinate è confermato
    dall'analisi grafica, che dice: il vettore $f(v)$ che si ottiene ruotando la
    sua lunghezza è proprio $\sqrt{2}$ volte la lunghezza di $v_2$:
    \begin{figure}[ht!]
      \centering
      \resizebox{5cm}{!}{\input{./img/mtxAsaplin5.tikz}}
      \caption{Quadrato composto da $v_1$ e $v_2$: analisi grafica}
      \label{fig:mtxAsaplin4-1}
    \end{figure}
    
    e con questo il primo punto è concluso.
  \item Sia $V=V_O^2$ lo spazio vettoriale dei vettori applicati in un punto $O$
    nel piano e sia $V_O^2\to V_O^2$ la proiezione ortogonale su una retta fissata
    $r$ passante per $O$ (in modo similare in quanto visto nella Figura \ref{fig:applindefes6}).

    Per calcolarne la matrice associata $M_B(f)$, si fissa una base $B=\{v_1,v_2\}$ di $V_O^2$
    come nella seguente figura
    \begin{figure}[ht!]
      \centering
      \resizebox{5cm}{!}{\input{./img/mtxAsaplin6.tikz}}
      \caption{Calcolo della matrice associata $M_B(f)$ con $B=\{v_1,v_2\}$ di $V_O^2$}
      \label{fig:mtxAsaplin5}
    \end{figure}

    Poi, bisogna proiettare $v_1$ ortogonalmente su $r$, ottenendo un vettore $v$ che
    sta sulla retta ed è lungo come metà dela diametro del quadrato in cui lati sono
    $v_1$ e $v_2$: essendo tale diagonale, per definizione di somma tra vettori,
    coincidente con $v_1+v_2$, quindi si ha che $f(v_1)=\frac{1}{2}(v_1+v_2)=
    \frac{1}{2}v_1+\frac{1}{2}v_2$; analogamente, come si vede dalla figura, anche
    proiettando $v_2$ sulla retta si ottiene lo stesso vettore $v$, quindi si ha
    anche $f(v_2)=\frac{1}{2}(v_1+v_2)=\frac{1}{2}v_1+\frac{1}{2}v_2$.
    \begin{figure}[ht!]
      \centering
      \resizebox{5cm}{!}{\input{./img/mtxAsaplin7.tikz}}
      \caption{Proiezione $v_1$ e $v_2$ nella forma $v_1+v_2$ }
      \label{fig:mtxAsaplin6}
    \end{figure}
    
    Si vede quindi che le coordinate di $f(v_1)$ rispetto a $B$ sono $\left(\frac{1}{2},
      \frac{1}{2}\right)$, e anche le coordiante di $f(v_2)$ rispetto a $B$ sono
    $\left(\frac{1}{2},\frac{1}{2}\right)$: disponendo tali coordinate rispettivamente
    sulla prima e sulla seconda colonna, come previsto dalla definizione di matrice associata,
    si ottiene
    \begin{equation*}
      M_B(f)=
      \begin{pmatrix}
        \frac{1}{2} & \frac{1}{2}\\
        \frac{1}{2} & \frac{1}{2}
      \end{pmatrix}
    \end{equation*}
    e la funzione $\mathds{R}^2 \to \mathds{R}^2$, corrisponde a
    \begin{equation}
      \label{eq:mtxAsaplin13}
      (x_1,x_2)\to
      \begin{pmatrix}
        \frac{1}{2}x_1+\frac{1}{2}x_2, \frac{1}{2}x_1+\frac{1}{2}x_2
      \end{pmatrix}
    \end{equation}
    Quindi dà una rappresentazione in coordinate della proiezione.

    Ad esempio, il vettore $v=-v_1+v_2$, che ha coordinate $(-1,1)$ rispetto
    a $B$, viene mandata in base alla (\ref{eq:mtxAsaplin13}) nel vettore di
    coordinate
    \begin{eqnarray*}
      \begin{pmatrix}
        \frac{1}{2}\cdot (-1)+\frac{1}{2}\cdot 1, \frac{1}{2}\cdot(-1)+\frac{1}{2}\cdot 1
      \end{pmatrix}=(0,0)
    \end{eqnarray*}
    ovvero nel vettore nullo $\vec{OO}$. infatti, Come si vede nella figura seguente, tale vettore
    appartiene alla retta passante per $O$ e ortogonale a $r$, e i vettori che giacciono su questa
    retta vengono chiaramente proiettati sul vettore nullo $\vec{OO}$.
    \begin{figure}[ht!]
      \centering
      \resizebox{5cm}{!}{\input{./img/mtxAsaplin8.tikz}}
      \caption{proiezione di $-v_1+v_2$ e del vettore nullo $\vec{OO}$}
      \label{fig:mtxAsaplin7}
    \end{figure}
  \item Come ultimo esempio, si prende $f:V_O^2\to V_O^2$ la riflessione rispetto a una retta
    fissata $r$ passate per $O$, ovvero l'endomorfismo che associa ogni vettore $\vec{OP}$ il
    suo simmetrico rispetto a $r$.

    Per calcolarne la matrice associata $M_B(f)$, considerando la stessa base $B=\{v_1,v_2\}$
    usata nell'esempio precedente
    \clearpage
    \begin{figure}[ht!]
      \centering
      \resizebox{5cm}{!}{\input{./img/mtxAsaplin6.tikz}}
      \caption{Calcolo della matrice associata $M_B(f)$ con $B=\{v_1,v_2\}$ di $V_O^2$}
      \label{fig:mtxAsaplin8}
    \end{figure}
    e si nota che quando si riflette $v_1$ rispetto a $r$ ottenendo $v_2$, ovvero $f(v_1)=v_2$,
    e analogamente quando si riflette $v_2$ rispetto a $r$, ottenendo $v_1$, ovvero $f(v_2)=v_1$.

    Quindi, riscrivendo $f(v_1)=v_2$ come $f(v_1)=0v_1+1v_2$ si vede che le coordinate di $f(v_2)$
    rispetto a $B$ sono $(1,0)$: disponendo tali coordinate in colonna, come previsto dalla
    definizione di matrice associata, si ottiene
    \begin{eqnarray*}
      M_B(f)=
      \begin{pmatrix}
        0 & 1 \\
        1 & 0
      \end{pmatrix}
    \end{eqnarray*}
    Se, dato sempre lo stesso endomorfismo, consideriando invece la base $B^\prime=\{v_1^\prime,
    v_2^\prime\}$ come da figura
    \begin{figure}[ht!]
      \centering
      \resizebox{5cm}{!}{\input{./img/mtxAsaplin9.tikz}}
      \caption{Esempio di endomosfismo con $B^\prime=\{v_1^\prime,v_2^\prime\}$}
      \label{fig:mtxAsaplin9}
    \end{figure}

    allora si ha che $f(v_1^\prime)=v_1^\prime$ e $f(v_2^\prime)=-v_2^\prime$, cioè $f(v_1^\prime)
    =1v_1^\prime+0v_2^\prime$ e $f(v_2)=0v_1^\prime+(-1)v_2^\prime$ e quindi
    \begin{eqnarray*}
      M_{B^\prime}(f)=
      \begin{pmatrix}
        1 & 0 \\
        0 & -1
      \end{pmatrix}
    \end{eqnarray*}
    Questo esempio illustra il fatto oovvio che la matrice associata dipende dalla scelta delle
    basi.
  \end{enumerate}
\end{es}

\section{Iniettività e suriettività di applicazioni lineari}
\label{sec:inietesuriet}

Il primo problema che verrà affrontato sulle applicazionio lineari è determinare quanto
una tale funzione è iniettiva, suriettiva o biiettiva.

\subsection{Richiemi generali}
\label{sec:richgen}

Ricordando che una funzione $f:A\to B$ tra due insiemi $A$ e $B$ si dice \emph{suriettiva}
se ogni elemento del codominio $B$ risulta essere immagine di qualche elemento di $A$,
ovvero, se per ogni $b\in B$ esiste un $a\in A$ tale che $f(a)=b$. Ad esempio, delle
funzioni rappresentate nel seguente disegno, la prima non è suriettiva, la seconda invece sì.
\clearpage
\begin{figure}[ht!]
  \centering
  \resizebox{10cm}{!}{\input{./img/inieEsurie1.tikz}}
  \caption{esempio di surietività}
  \label{fig:ricgendelfab}
\end{figure}
Un modo alternativo di dire che una funzione è suriettiva, è proprio quello di fare riferimento
alla cosiddetta \textit{immagine} $I_m(f)$ di $f$: per definizione, l'immagine di una funzione
$f:A\to B$ è il sotoinsieme di $B$ costituito da tutti gli elementi che sono immagine di qualche
elemento di $A$\footnote{In riferimento alle figure, quegli elemnti (raggiunti da una freccia che
  proviene da $A^{\prime\prime}$)}, ovvero
\begin{eqnarray*}
  I_m(f)=\{b\in B\text{ | }b=f(a) \text{ per qualche } a\in A\}
\end{eqnarray*}
Ad esempio, la funzione a sinistra nella figura precedente ha $I_m(f)=\{a,b\}$, mentre la funzione
a destra $I_m(f)=\{a,b,c\}$: una funzione è suriettiva esattamente quando $I_m(f)=B$, ovvero l'immagine
coincide con tutto il codominio\footnote{dire $I_m(f)=B$ significa effetti dire che ogni elemento di
  $B$ è immagine di elemento di $A$}.\\
Una funzione $f:A\to B$ si dice invece \textit{iniettiva} se non succede che due elementi diversi di $A$
abbiano la stessa immagine\footnote{in formula, $f$ è iniettiva se $a_1\neq a_2\Rightarrow
  f(a_1)\neq f(a_1)$ o equivalentemente $a_1= a_2\Rightarrow f(a_1)= f(a_1)$}. Ad esempio, delle funzioni
rappresentate nella seguente figura, la prima non è iniettiva\footnote{in quanto nonostante $1\neq 2$
  si ha $f(1)=f(2)=a$}, la seconda sì.
\begin{figure}[ht!]
  \centering
  \resizebox{10cm}{!}{\input{./img/inieEsurie2.tikz}}
  \caption{esempio di iniettività}
  \label{fig:ricgendelfab2}
\end{figure}

La nozione di iniettività può essere riformulata tramite il concetto di \textit{controimmagine}: dato
un elemento $b$ del codominio $B$, la sua controimmagine, denotata $f^{-1}(b)$, è l'insieme di tutti gli
elementi di $A$ che hanno $b$ come immagne, ovvero
\begin{eqnarray*}
  f^{-1}(b)=\{a\in A \text{ | } f(a)=b\}
\end{eqnarray*}
Dal momento che una funzione è iniettiva quando non esistono due elementi diversi che la stessa immagine,
dire che tutte le controimmagine che non siano vuote\footnote{se la funzione non è suriettiva, ci
  saranno elementi $b\in B$ tali che non esiste nessun $a\in A$ con $f(a)=b$, e quindi la cui
  controimmagine $f^{-1}(b)$ non ha elementi.} hanno un solo elemento.\\
Ad esempio per la funzione nella figura precedente si ha $f^{-1}(a)=\{1,2\},f^{-1}(d)=\diameter,
f^{-1}(c)=\{3\}$: essa non è iniettiva in quanto la controimmagine di $a$ ha due elementi. Per la
funzione a destra, invece, si ha $f^{-1}(a)=\{1\},f^{-1}(b)=\{2\}, f^{-1}(c)=\{3\},f^{-1}(d)=\diameter$:
essa è iniettiva in quanto le controimmagini non vuote hanno tutte un solo elemento. Infine, una funzione
si dice invece \textit{biiettiva} se è sia iniettiva che suriettiva. Ad esempio, la funzione
rappresentata nella seguente figura è biiettiva.
\clearpage
\begin{figure}[ht!]
  \centering
  \resizebox{10cm}{!}{\input{./img/inieEsurie3.tikz}}
  \caption{caso di biiettività}
  \label{fig:ricgendelfab3}
\end{figure}

\subsection{Suriettività di applicazioni lineari}
\label{sec:suriappllin}
avendo detto che una funzione $f:A\to B$ è suriettiva se e solo se per
ogni elemento $b\in B$ esiste un $a\in A$ tale che $f(a)=b$, ovvero
equivalentemente se e solo se la sua immagine $I_m(f)$ coincide con tutto
il codominio. Quindi, per capire se un'applicazione è suriettiva,
bisogna determinare l'insieme $I_m(f)$.\\
Nel csaso di un'applicazione lineare $f:V\to W$, vale la seguente
importante
\begin{prop}
  \label{prop:suriappllin1}
  Sia $f:V\to W$ un'applicazione lineare. Allora $I_m(f)$ è un
  sottospazio vettoriale di $W$.
\end{prop}
\begin{proof}
  Bisogna verificare che $I_m(f)$ è chiuso rispetto alla somma e al
  prodotto per scalari. Per la prima proprietà bisogni prevedere $w,
  w^\prime\in I_m(f)$ e vedere se $w+w^\prime\in I_m(f)$ , per definizione
  di $I_m(f)$ e vedere se $w,w^\prime\in I_m(f)$. Ora, se
  $w,w^\prime\in I_m(f)$, per definizione di $I_m(f)$ significa che
  esistono un vettore $v\in V$ tale che $w=f(v)$ e un vettore
  $v^\prime\in V$ tale $w^\prime=f(v^\prime)$. Ma allora, sfruttando il
  fatto che $f$ è lineare, si ha
  \begin{eqnarray*}
    w+w^\prime=f(v)+f(v^\prime)=f(v+v^\prime)
  \end{eqnarray*}
  che afferma, anche che $w+w^\prime$ è immagine un elemento del dominio
  (cioè $v+v^\prime$) e quindi $w+w^\prime\in I_m(f)$.
  Per la chiusura rispetto al prodotto per scalari, bisogna verificare
  che se $w\in I_m(f)$ $c\in \mathds{K}$, allora $cw\in I_m(f)$. Ma, come
  prima, se $w\in I_m(f)$ allora per definizione di $I_m(f)$ esiste un
  vettore $w=f(v)$, e quindi, usando sempre il fatto che $f$ e lineare,
  \begin{eqnarray*}
    cw=cf(v)=f(cv)
  \end{eqnarray*}
  che ci dice anche $cw$ è immagine di un elemento del dominio (cioè
  $cv$) e quindi $cw\in I_m(f)$.
\end{proof}
Il fatto che $I_m(f)$ sia un sottospazio vettoriale ci dice che per
dterminarla è possibile trovare un sistema di generatori o una base.
Questo si fa facilmente grazie alla seguente
\begin{prop}
  \label{prop:suriappllin2}
  Sia $f:V\to W$ un'applicazione lineare e siano $v_1,\dots, v_n$
  generatori di $V$. Allora le immagini $f(v_1),\dots,f(v_n)$ generano
  $I_m(f)$\footnote{in simboli, $I_m(f)=(f(v_1),\dots,f(v_n))$}
\end{prop}
\begin{proof}
  Per definizione di generatori, bisogna verificare che ogni vettore
  $w\in I_m(f)$ si può scrivere come combinazione lineare dei vettori
  $f(v_1),\dots,f(v_n)$. Si sa che un vettore $w\in I_m(f)$ è tale che
  $w=f(v)$ per qualche vettore $v\in V.$ Ma essendo per ipotesi $v_1,
  \dots,v_n$ generatori di $V$, il vettore $v$ potrà essere scritto come
  loro combinazione lineare $v=x_1v_2+\cdots+x_nv_n$. Quindi, sfruttando
  la linearità di $f$ si ha
  \begin{eqnarray*}
    w=f(v)=f(x_1v_1+\cdots+x_nv_n)=x_1f(v_1)+\cdot+x_nf(v_n)
  \end{eqnarray*}
  che dimostra proprio che $w$ si scrive come combinazione lineare di
  $f(v_1),\dots,f(v_n)$.
\end{proof}
A questo punto, per definire se un'applicazione lineare $f:V\to W$ è
suriettiva, basta scegliere dei generatori $v_1,\dots,v_n$ di $V$,
prendere le loro immagini $f(v_1),\dots,f(v_n)$ che, formano un insieme
di generatori di $I_m(f)$, e poi estrarre da quest'ultima insieme una
base eliminando gli eventuali vettori che sono dipendenti dai rimanenti:
contando i vettori della base ottenuta, si saprà la dimensione di
$I_m(f)$ e quindi $f$ sarà se e solo se\footnote{Questa affermazione è
  giustificata dal fatto, che non è stato possibile dimostrare, che se
  $S$ è un sottospazio vettoriale di uno spazio vettoriale $V$, allora
  $\dim (V)$ e le dimensioni ccoincidono se e solo se $S=V$.}
$\dim(I_m(f))=\dim(W)$.\\
È il caso di vedere come tale criterio di suriettività si rivela
particolarmente utile e di semplice applicazione nel caso
dell'applicazione nel caso dell'applicazione lineare $L_A:\mathds{K}^n
\to \mathds{K}^m$ determinata da un matrice $A$, ovvero della forma data
dalla (\ref{eq:mtxAsaplin7}). Ora, se come generatore del dominio
$V=\mathds{K}^n$ si scelgono i vettori della base canonica $v_1=
(1,0,\dots,0),v_2=(0,1,\dots,0),v_n=(0,0,\dots,1)$, dalla
(\ref{eq:mtxAsaplin7}) si ha
\begin{eqnarray*}
  f(v_1)=L_A
  \begin{pmatrix}
    1\\
    0\\
    \vdots\\
    0
  \end{pmatrix}=
  \begin{pmatrix}
    a_{11}\\
    a_{21}\\
    \vdots\\
    a_{m1}
  \end{pmatrix},f(v_2)=L_A
  \begin{pmatrix}
    0\\
    1\\
    \vdots\\
    a_{m2}
  \end{pmatrix},\dots,f(v_n)=L_A
  \begin{pmatrix}
    0\\
    0\\
    \vdots\\
    1
  \end{pmatrix}=
  \begin{pmatrix}
    a_{1n}\\
    a_{2n}\\
    \vdots\\
    a_{mn}
  \end{pmatrix}
\end{eqnarray*}
cioè $f(v_1),f(v_2),\dots,f(v_n)$ sono le colonne $C_1,C_2,\dots,C_n$
della matrice $A$ che determina l'applicazione. Quindi, in base alla
Proposizione \ref{prop:suriappllin2}, si ha $I_m(f)=(C_1,C_2,\dots,C_n)$
e la funzione è suriettiva se e solo se $\dim(C_1,C_2,\dots,C_n)=
\dim(\mathds{K}^m)=m$. Ma poiché la dimensione del sottospazio generato
dalle colonne di una matrice è per definizione il suo rango, è possibile
concludere che \textit{un'applicazione del tipo (\ref{eq:mtxAsaplin7})
  è suriettiva se e solo se il rango di $A$ uguale a $m$}.

\subsection{Iniettività di applicazioni lineari}
\label{sec:iniediappllin}

Mentre, nel caso della suriettività si è visto che per verificare se una
funzione $f$ è suriettiva basta controllare un solo sottoinsieme del
codominio\footnote{l'immagine $I_m(f)$}, in generale per verificare se
$f$ e iniettiva bisogna controllare tutte le controimmagini degli
elementi del codominio e verificare che queste, quando non sono vuote,
hanno un solo elemento.\\
Ora, per una generica funzione $f:A\to B$ le controimmagine degli
elementi di $B$ sono sottoinsimi del tutto indipendenti tra loro: come
nella seguente figura
\begin{figure}[ht!]
  \centering
  \resizebox{8cm}{!}{\input{./img/inieEsurie4.tikz}}
  \caption{Iniettiità di applicazioni lineari}
  \label{fig:iniediappllin}
\end{figure}

può accadere che un elemento abbia controimmagine costituita da un solo
elemento ma altri abbiano controimmagine costituita da più elementi.\\
Mentre, le applicazione linari hanno il particolare comportamento per cui
le controimmagini degli elementi di $B$, se non solo vuote, o sono
\textit{tutte} costituite da un solo elemento o hanno \textit{tutti} più
di un elemento: quindi basta controllare una sola controimmagine non vuota
come sono fatte tutte le altre. Più precisamente si ha la sequente
\begin{prop}
  \label{prop:iniediappllin1}
  Sia $f:V\to W$ un'applicazione lineare. Allora valgono i seguenti fatti:
  \begin{enumerate}
  \item la controimmagine $f^{-1}(\bar{0})=\{v\in V \text{ | }
    f(v)=\bar{0}\}$ del vettore nullo di $W$ è un sottospazio vettoriale
    di un sottospazio vettoriale $V$ (detto \textit{nucleo di} $f$ e
    denotato $N(f)$)
  \item per ogni $w_0\in W$, la controimmagine $f^{-1}(w_0)$, se non è
    vuota, è sottospazio affine di $V$, e più precisamente
    \begin{eqnarray*}
      f^{-1}(w_0)=v_0N(f)=\{v_0+n \text{ | } n\in N(f)\}
    \end{eqnarray*}
    dove $v_0$ è un qualunque elemento fissato di $f^{-1}(w_0)$.
  \end{enumerate}
\end{prop}
\begin{proof}
  Per dimostrare il primo punto, bisogna osservare che il nucleo di $f$
  non è mai vuoto, in quanto il vettore nullo di $V$ è sicuramente tale
  che $f(\bar{0})=\bar{0}$: infatti, è possibile concepire il vettore
  nullo $\bar{0}$ di $V$ come $0v$ e quindi, sfruttando la linearità di
  $f$, si ha $f(\bar{0})=f(0v)=0f(v)=\bar{0}$.
  È il momento di dimostrare $N(f)$ è chiuso rispetto alla somma e al
  prodotto per scalari. Siano $v,v^\prime$ due vettori di $N(f)$, cioè
  $f(v)=\bar{0}$ e $f(v^\prime)=\bar{0}$. Allora, essendo $f$ lineare,
  \begin{eqnarray*}
    f(v+v^\prime)=f(v)+f(v^\prime)=\bar{0}+\bar{0}=\bar{0}
  \end{eqnarray*}
  e quindi anche $v+v^\prime\in N(f)$: questo si dice che $N(f)$ è chiuso
  rispetto alla somma. Dati invece un vettore $v$ del
  nucleo\footnote{quindi $f(v)=\bar{0}$} e uno scalare $c\in \mathds{K}$,
  allora, sempre per la linearità di $f$,
  \begin{eqnarray*}
    f(cv)=cf(v)=c\bar{0}=\bar{0}
  \end{eqnarray*}
  ovvero $cv\in N(f)$: questo dice che $N(f)$ è chiuso rispetto al
  prodotto per scalari. Con questo il punto 1 è stato dimostrato.\\
  Per dimostrare il secondo punto, ovvero l'uguaglianza $v_0+N(f)=
  f^{-1}(w_0)$, bisogna dimostrare che ogni elemento di $v_0+N(f)$ sta
  nella controimmagine $f^{-1}(w_0)$ di $w_0$ (ovvero $v_0+N\subseteq f^{-1}(w_0)$), e viceversa
  che ogni elemento di $f^{-1}(w_0)$ appartiene a $v_0+N(f)$\footnote{ovvero l'inclusione opposta
    $f^{-1}(w_0) \subseteq v_0+N(f)$}.\\
  Per dimostrare la prima inclusione, considerando il generico elemento di $v_0+N(f)$, cioè, per
  definizione di sottospazio affine, un vettore $v$ del tipo $v=v_0+n$, con $n\in N(f)$. Allora
  \begin{eqnarray*}
    f(v)=f(v_0+n)=f(v_0)+f(n)=f(v_0)+\bar{0}=f(v_0)=w_0
  \end{eqnarray*}
  Avendo quindi dimostrato che $f(v)=w_0$, cioè $v$ appartiene alla controimmagine $f^{-1}(w_0)$ di
  $w_0$. \\
  Per dimostrare la seconda inclusione, considerando un qualunque elemento $v$ della controimmagine
  di $w_0$, cioè $f(v)=w_0$. Essendo $w_0=f(v_0)$, si ha quindi $f(v)=f(v_0)$, da cui, portando a prima
  membro, $f(v)-f(v_0)=\bar{0}$. Essendo $f$ lineare, quest'ultima uguaglianza può essere riscritta
  come $f(v-v_0)=\bar{0}$, il che dice che il vettore $v-v_0$ appartiene al nucleo $N(f)$ di $f$.
  Ma allora, osservando che chiaramente $v=v_0+(v-v_0)$, vedendo che $v$ si decompone proprio come somma
  di $v_0$ e di un elemento del nucleo $N(f)$, cioè $v\in v_0+N(f)$.
\end{proof}
La proposizione appena dimostrata afferma in pratica che tutte le controimmagini non vuote di
un'applicazione lineare $f$ sono ``copie'' o traslati del nucleo $N(f)$: per illustrare ciò,
si consideri ad esempio lo spazio $V_o^2$ dei vettori nel piano applicati in $O$ e l'applicazione
lineare $f:V_o^2\to V_o^2$ data dalla proiezione su una retta $r$ fissata.\\
Come si vede, un vettore viene proiettato sul vettore nullo $\vec{OO}$\footnote{cioè appartiene al
  nucleo $N(f)$ della funzione} se e solo se appartiene alla retta passante per $O$ e ortogonale a $r$,
come i vettori $\vec{OP}, \vec{OQ}$ della seguente figura
\begin{figure}[ht!]
  \centering
  \resizebox{7cm}{!}{\input{./img/inieEsurie5.tikz}}
  \caption{Proiezione sul vettore nullo $\vec{OO}$ mediante retta $O$}
  \label{fig:iniediappllin2}
\end{figure}
Ma, i vettori che stanno su una retta per $O$ formano un sottospazo vettoriale: questo conferma che
il nucleo $N(f)$ è un sottospazio vettoriale.\\
Per verificare che le controimmagini non vuote sono copie o traslat del nucleo, considerando come nella
figura seguente
\clearpage
\begin{figure}[ht!]
  \centering
  \resizebox{5cm}{!}{\input{./img/inieEsurie6.tikz}}
  \caption{Immagine $f$ con $\vec{OQ},\vec{OR},\vec{OP}$ e $\vec{OP}_0$}
  \label{fig:iniediappllin3}
\end{figure}
un qualunque vettore $\vec{OQ}$ che stia nell'immagine di $f$, definendo $\vec{OQ}=f(\vec{OP}_0)$:
la sua controimagine, oltre che da $\vec{OP}_0$, è data da tutti i vettori $\vec{OP}$ che vengono
proiettati su $\vec{OQ}$, cioè, come si vede nella figura, tutti i vettori che hanno secondo estremo
sulla retta ortogonale a $r$ e passante per $Q$.\\
Ognuno di tali vettori $\vec{OP}$ si decompone come somma di $\vec{OP}_0$ più un vettore $\vec{OR}$
appartenente al nucleo $N(f)$: quindi, come previsto dalla Proposizione \ref{prop:iniediappllin1},
la controimmagine $f^{-1}(\vec{OP})$ è data dal sottospazio affine $\vec{OP}_0+N(f)$, traslato della
retta ortognonale a $r$ e passante per $O$ che rappresenta il nucleo. La Proposizione
\ref{prop:iniediappllin1} ha come immediato corollario il seguente criterio necessario e sufficiente
di iniettività per un'applicazione lineare:
\begin{corol}
  \label{corol:iniediappllin1}
  Un'applicazione lineare $f:V\to W$ è iniettiva se e solo se $N(f)=\{\bar{0}\}$. 
\end{corol}
\begin{proof}
  Un'applicazione lineare è iniettiva se e solo se la controimmagine di ogni elemento $w_0\in W$, se
  non è vuota\footnote{cosa che succede se $w_0$ non sta nell'immagine dell'applicazione}, contiene
  un solo elemento. Ma poiché, come visto nella proposizione, la controimmagine di ogni elemento $w_0$
  dell'immagine è del tipo $v_0+N(f)$, allora questa conterrà un solo elemento $v_0$ esattamente quando
  il nucleo contiene il solo vettore nullo $\bar{0}$, cioè $N(f)=\{\bar{0}\}$.
\end{proof}
Si noti che il nucleo sia un sottospazio vettoriale dice che si può parlare di base e dimensione del
nucleo, e che si possono riformulare il criterio di iniettività per un'applicazione lineare $f$ come
segue: $f$ \emph{è iniettiva se e solo se} $\dim(N(f))=0$\footnote{infatti, un sottospazio ha dimensione
  0 se e solo se è $\{\bar{0}\}$}.\\
Riassumendo quanto visto finora, come dimostrato per veriricare l'iniettività di un'applicazione
lineare $f$ si deve guardare la dimostrazione $\dim(N(f))$ del nucleo $N(f)$ di $f$, mentre per
verificare la suiriettività di $f$ si deve guardare la dimensione $\dim(I_m(f))$ dell'immagine
$I_m(f)$ di $f$.\\
Una cosa che appare lampante è che in realtà sarà sufficiente calcolare una sola di queste dimensioni
per determinare automaticamente anche l'altra. Infatti, queste dimensioni sono collegate dalla formula
del sequente risultato, dove anche \emph{teorema della dimensione} o \emph{teorema nullità più rango}.
\begin{teo}
  \label{teo:iniediappllin1}
  Sia $f:V\to W$ un'applicazione lineare, con $\dim(V)$ finita. Allora
  \begin{eqnarray}
    \label{eq:iniediappllin1-1}
    \dim(N(f))+\dim(I_m(f))=\dim(V).
  \end{eqnarray}
\end{teo}
\begin{proof}
  Supponendo che $\dim(N(f))=s$ e sia $v_1,\dots,v_s$ una base $N(f)$. Se $\dim(V)=n$, allora si
  può\footnote{Il fatto che data una base di un sottospazio, questa possa sempre essere completata
    a una base di tutto lo spazio aggiungendo dei vettori è in effetti un teorema detto \emph{teorema
      del completamento}.} aggiungere alla base del nucleo $n-s$ vettore $v_{s+1},\dots,v_n$ in modo
  che\\ $\{v_1,\dots,v_s,v_{s+1},\dots,v_n\}$ sia una base di $V$.\\
  Ora, dimostrando che le immagini $f(v_{s+1}),\dots,f(v_n)$ di questi ultimi $n-s$ vettori formano una
  base di $I_m(f)$:
  \begin{quote}
    Questo implicherà che $\dim(I_m(f)=n-s$, che assieme a $\dim(N(f))=s$ e $\dim(V)=n$ dà $\dim(N(f))+
    \dim(I_m(f))=s+(n-s)+\dim(V)$, cioè la formula.
  \end{quote}
  Per dimostrare che $\{f(v_{s+1}),\dots,f(v_n)\}$ è una base di $I_m(f)$, bisogna dimostrare che i
  vettori generano $I$ e sono linearmente indipendenti. In effetti, sapendo che essendo $v_1,\dots,v_s,
  v_{s+1}\dots, v_n$ una base e quindi un insieme di generatori di $V$, le immagini $f(v_1),\dots,f(v_s),
  f(v_s+1),\dots,f(v_n)$ generatori $I_m(f)$; ma in questi generatori $f(v_1),\dots,f(v_s)$ sono uguali
  al vettore nullo $\bar{0}$, in quanto $v_1,\dots,v_s$ sono i vettori della base del nucleo fissata
  inzialmente. Quindi possono essere eliminati della lista $f(v_1),\dots,f(v_s), f(v_s+1),\dots,f(v_n)$
  dei generatori, concludendo che per generare $I_m(f)$ bastano $f(v_s+1),\dots,f(v_n)$.\\
  Ora dimostrando che $f(v_s+1),\dots,f(v_n)$ sono linearmente indipendenti. Bisogna dimostrare che se
  $c_{s+1}f(v_{s+1})+\cdots+c_nf(v_n)=\bar{0}$ allora $c_{s+1}=0,\dots,c_n=0$.\\
  In effetti, sfruttando il fatto che $f$ è lineare è possibile riscrivere l'uguaglianza
  $c_{s+1}f(v_{s+1})+\cdots+c_nf(v_n)=\bar{0}$ come $f(c_{s+1}v_{s+1}+\cdots+c_nv_n)=\bar{0}$: ma questa
  uguaglianza dice che il vettore $c_{s+1}v_{s+1}+\cdots+c_nv_n$ appartiene il nucleo $N(f)$, e quindi
  esso può esso può essere scritto come combinazione lineare di $v_1,\dots,v_s$\footnote{che
    del nucleo costituiscono una base}:
  \begin{eqnarray*}
    c_{s+1}v_{s+1}+\cdots+c_nv_n=c_1v_1+\cdots+c_sv_s.
  \end{eqnarray*}
  Portando tutto a primo membro in questa uguaglianza si ottiene
  \begin{eqnarray*}
    c_{s+1}v_{s+1}+\cdots+c_nv_n-c_1v_1-\cdots-c_sv_s=\bar{0}
  \end{eqnarray*}
  ovvero una combinazione lineare uguale al vettore nullo dei vettori $v_1,\dots,v_s, v_{s+1}\dots, v_n$:
  essendo questi vettori indipendenti\footnote{sono i vettori che formano la base completa di $V$}
  necessariamente tutti i coefficianti $c_1,\dots,c_s,c_{s+1},\dots,c_n$ sono uguali a zero, e in
  particolare $c_{s+1}=0,\dots,c_n=0$, che è quello che restava da dimostrare.
\end{proof}
Vedendo ora alcune notevoli conseguenze della formula (\ref{eq:iniediappllin1-1}):
\begin{corol}
  \label{corol:iniediappllin2}
  Sia $f:V\to W$ un'applicazione lineare, con $\dim (V)$ finita.
  Allora valgono le tre seguenti
  \begin{enumerate}[label=(\roman*)]
  \item se $\dim(V)>\dim(W)$ allora $f$ non è iniettiva;
  \item se $\dim(V)<\dim(W)$ allora $f$ non è suriettiva;
  \item se $\dim(V)=\dim(W)$ allora $f$ è iniettiva se e solo se è suriettiva (\texttt{caso di
      biiettività})
  \end{enumerate}
\end{corol}
\begin{proof}
  Quei di seguito verrano dimostrati i tre punti definiti nel Corollario \ref{corol:iniediappllin2}:
  \begin{enumerate}[label=(\roman*)]
  \item Per assurdo, se la funzioni $f$ fosse iniettiva, il suo nucleo, come visto nel Corollario,
    sasrebbe nullo, ovvero $\dim(N(f))=0$. Sostituendo questo nella formula (\ref{eq:iniediappllin1-1}),
    si avrà $\dim(V)=\dim(I_m(f))$. Ma essendo $I_m(f)$ un sottospazio di $W$, la sua dimensione è
    sicuramente minore o uguale a $\dim(W)$, contro l'ipotesi che $\dim(V)>\dim(W)$. Quindi $f$ non può
    essere iniettiva.
  \item Supponendo per assurdo che la funzione $f$ sia suriettiva: allora, essendo $I_m(f)=W$, si avrà
    $\dim(I_m(f))=\dim(W)$ che, sostituita nella formula (\ref{eq:iniediappllin1-1}), dà $\dim (V)=
    \dim(W)+\dim(N(f))$. Poiché $\dim(N(f))$ è un numero reale positivo o nullo, questa uguaglianza
    implica che $\dim(V)\leq \dim(W)$, contro l'ipotesi che $\dim(V)<\dim(W)$. Quindi $f$ non può essere
    suiettiva.
  \item Vicerversa, se $f$ è suriettiva, $\dim(V)=\dim(W)$ e quindi la formula
    (\ref{eq:iniediappllin1-1}) si riduce a $\dim(V)=\dim(N(f))+\dim(W)$. Essendo per ipotesi la
    dimensione di $V$ uguagle a quella di $W$, il primo membro $\dim(V)$ si semplifica con l'addendo
    $\dim(W)$ del secondo membro, e quindi rimane $=\dim(N(f))$, che implica che $f$ è iniettiva.
  \end{enumerate}
\end{proof}
Ad esempio, un'applicazione lineare $f:\mathds{R}^3\to\mathds{R}^2$ non può mai essere iniettiva
(ma può essere suriettiva); un'applicazionelineare $f:\mathds{R}^3\to\mathds{R}^4$ non è sicuramente
suriettiva (ma può essere iniettiva); un'applicazione lineare $f:\mathds{R}^3\to\mathds{R}^3$ o è
contemporaneamente iniettiva e suriettiva o nessuna della due: basta mostrare che vale una delle due
proprietà e automaticamente varrà anche l'altra.\\
Ora, supponendo di aver determinato grazie ai risultati precedenti che un'applicazione lineare
$f$ data è biiettiva.

\section{Composizione di applicazioni, inversa e prodotto di matrici}
\label{sec:Compinveeproddimatrici}

Ricordiamo che, data una funzione $f:X\to Y$ tra due insiemi, questa si
dice \textit{invertibile} se esiste una funzione $g:Y\to X$ (detta
appunto l'inversa di $f$) tale che
\begin{equation}
  \label{eq:Compinveeproddimatrici1}
  f \circ g=id_{y}, \text{ } g\circ f=id_{x}
\end{equation}
e si denota che con \textit{id} la funzione che manda ogni elemento in se
stesso\footnote{Più precisamente, $id_{x}$ è la funzione $X\to X$ che
  manda ogni elemento di $x$ in se stesso e $id_{y}$ denota la funzione
  $Y\to Y$ che manda ogni elemento di $Y$ in se stesso.} e con il simbolo
$\circ$ la composizione di funzioni, ovvero l'operazione che consiste
nell'applicare prima una fuzione e poi l'altra: più precisamente,
ricordiando che ogni volta che si hanno due funzioni $f:X\to Y$ e
$g:Y\to Z$, tali che \textit{il codominio della prima coincida con il
  della seconda,} allora, per ogni elemento che $Y$ è anche, per ogni
elemento $x\in X$, si può applicare prima $f$ ottenendo $f(x)\in Y$,
e poi dal momento che $Y$ è anche il dominio della $g$ si può applicare
la $g$ a $f(x)$. In questo modo si ottiene una nuova funzione che
associa a ogni elemento di $X$ un elemento di $Z$:
\begin{equation*}
  \underset{x\to g(f(x))}{f:X\to Z}
\end{equation*}
Quindi, le (\ref{eq:Compinveeproddimatrici1}) significano che una
funzione $f:X\to Y$ è invertibile se esiste una funzione $g:Y\to X$
tale $g(f(x))=x$ $x\in X$ e $f(g(y))=y$ per ogni $y\in Y$.\\
Ora, si può vedere che le uniche funzioni di $f$ invertibili sono
quelle biiettive. Ad esempio, considerando $X=\{1,2,3\}$, $Y=\{a,b,c\}$
e la funzione $f:X\to Y$ biiettiva, che ha come inversa la funzione
$g:Y\in X$ rappresentata nella figura:
\begin{figure}[ht!]
  \centering
  \resizebox{15cm}{!}{\input{./img/invEprodMatrice.tikz}}
  \caption{Funzione $f$ inversa di $X=\{1,2,3\}$ e $Y=\{a,b,c\}$}
  \label{fig:Compinveeproddimatrici1}
\end{figure}

Infatti, come si vede fin da subito, si ha
\begin{equation*}
  \begin{matrix}
    (g\circ f)(1)=g(f(1))=g(a)=1\\
    (g\circ f)(2)=g(f(2))=g(b)=2\\
    (g\circ f(3)=g(f(3))=g(c)=3
  \end{matrix}
\end{equation*}
ovvero $g\circ f:\{1,2,3\} \to \{1,2,3\}$ è la funzione che manda ogni elemento dell'insieme
$X=\{1,2,3\}$ in se stesso (ovvero la funzione identica $id_{X}$ di X) e analogamente
\begin{equation*}
  \begin{matrix}
    (g\circ f)(a)=g(f(a))=g(1)=a\\
    (g\circ f)(b)=g(f(b))=g(2)=b\\
    (g\circ f(c)=g(f(c))=g(3)=c
  \end{matrix}
\end{equation*}
ovvero $g\circ f:\{a,b,c\} \to \{a,b,c\}$ è la funzione che manda ogni elemento dell'insieme
$Y=\{a,b,c\}$ in se stesso (cioè la funzione identica $id_Y$). Per giusticare l'affermazione che le
funzioni biiettiva sono \textit{le sole} a essere invertibili, considerando ad esempio la funzione
$f$ rappresentata nella seguente figura, che è iniettiva ma non suriettiva
\clearpage
\begin{figure}[ht!]
  \centering
  \resizebox{15cm}{!}{\input{./img/invEprodMatrice.tikz}}
  \caption{Funzione $f$ non invertibile di $X=\{1,2,3\}$ e $Y=\{a,b,c,d\}$}
  \label{fig:Compinveeproddimatrici2}
\end{figure}
Si vede subito che $g\circ f$ è una funzione identica di $\{1,2,3\}$, ma $f\circ g$ non è una funzione identica
di $\{a,b,c,d\}$ in quanto pur avendo
\begin{equation*}
  \begin{matrix}
    (g\circ f)(1)=g(f(1))=g(a)=1\\
    (g\circ f)(2)=g(f(2))=g(b)=2\\
    (g\circ f(3)=g(f(3))=g(c)=3
  \end{matrix}
\end{equation*}
non si ha, invece,
\begin{equation*}
  \begin{matrix}
    (g\circ f)(a)=g(f(a))=g(1)=a\\
    (g\circ f)(b)=g(f(b))=g(2)=b\\
    (g\circ f(c)=g(f(c))=g(3)=c
  \end{matrix}
\end{equation*}
Come si evince, dalla Figura \ref{fig:Compinveeproddimatrici2}, è presente un equivalenza in più che rimanda
ad un elemento già puntato del gruppo $X$, $f(g(d))=f(1)=a$ ovvero $f\circ g$ non manda $d$ in se stesso.

Si osservi che non c'è alcun modo di modificare $g$ in modo che $f\circ g$ mandi ogni elemento di $\{a,b,c,d\}$ in
se stesso: qualunque valore venga assegnato a $d$ non sarà mai $f(g(d))=d$, perché $g(d)$ dovrebbe essere un elemento
mandato da $f$ di $d$, ma non esiste nssun elemento di $\{1,2,3\}$ che viene mandato da $f$ in $d$.\\
Espresso in altri termini, il modo per cui l'uguaglianza $f\circ g=id$ non può mai essere verificata è la non
surietività d $f$. Si dice che $f$ ha un'\textit{inversa sinistra}\footnote{cioè $g\circ f=id$ è verificato} ma non
ammette un'\textit{inversa destra} (cioè la $f\circ g=id$ non può valere per nessuna $g$). Analogamente, si prendano
$f:\{1,2,3\}\to \{a,b\}$ e $g:\{a,b\}\to \{1,2,3\}$ definite come dalla figura seguente
\begin{figure}[ht!]
  \centering
  \resizebox{15cm}{!}{\input{./img/invEprodMatrice3.tikz}}
  \caption{Funzione $f$ non invertibile di $X=\{1,2,3\}$ e $Y=\{a,b\}$}
\label{fig:Compinveeproddimatrici3}
\end{figure}

Anllora, si vede fin da subito che $f\circ g$ è una funzione identica di $\{a,b\}$, ma $g\circ f$ non è la funzione
identica di $\{1,2,3\}$ in quanto pur essendo $g(f(1))=g(a)=1$ e $g(f(2))=g(b)=2$, si ha oltre tutto un $g(f(b))=g(b)=2$,
ovvero $g\circ f$ non manda 3 in se stesso.\\
Si osservi che anche qui non c'è alcun modo di modificare $g$: se si avesse posto $g(b)=23$ si otterrebbe
$g(f(3))=g(b)=3$ ma stavolta sarebbe stato $g(f(2))=g(b)=3$ saranno uguali e non potranno mai essere il
primo 2 e il secondo 3. In altri termini, il motivo per cui non esiste un'inversa sinistra di $f$ è dovuto
alla non iniettività di $f$: la $f$ ha un'inversa sinistra.

\begin{es}
  \label{es:Compinveeproddimatrici1}
  La funzione $f:\mathds{R}\to \mathds{R}$ data da $f(x)=x^2$, non essendo nè iniettiva\footnote{due numeri uno
    l'opposto dell'altro hanno lo stesso quadrato} nè suiettiva\footnote{i numeri negativi non sono quadrati di nessun
    numero reale} non ha né inversa sinistra né inversa a destra. In efetti, la candidata ad essere inversa di $f$,
  la radice quadrata $g(x)=\sqrt{x}$, da una parte non soddisfatta $g(f(x))=x$ se $x$ è negativa perché in tal caso
  $g(x^2)=\sqrt{x^2}=\abs{x}$, e $\abs{x}\neq x$ se essa è negativa $(\abs{-2}=+2\neq -2)$; dall'stra parte $f(g(y))=y$
  non ha senso se $y$ è negativo in quanto in tal caso $g(y)=\sqrt{y}$ non è neanche un numero reale. Per eliminare i
  due problemi (e rendere $g$ l'inversa di $f$) bisogna togliere i numeri negativi sia dal dominio che dal codominio di
  $f$, ovvero restringerla a una funzione $f:\mathds{R}_{\geq 0}\to \mathds{R}_{\geq0}$, dove $\mathds{R}_{\geq 0}$ denota
  l'insieme dei numeri non negativi: ma così facendo in effetti si fa proprio in modo che diventi una funzione
  biiettiva\footnote{non ci sono due elementi del dominio con lo stesso quadrato, e ogni elemento del codominio è
    quadrato di qualcosa}.
\end{es}
Ora, come spiegato sopra, si pone il problema di calcolare, se esiste, l'inversa di un applicazione lineare data. Prima
di fare ciò, dal momento che l'inversa è definita tramite la composizione, bisogna vedere cosa succede quando si compongono
due applicazioni lineari. In particolare, poiché ogni applicazione lineare può essere sempre tradotta in una formuala della
tipologia (\ref{eq:mtxAsaplin7}), e adesso sarà possibile osservare cosa succede quando si compongono due applicazioni di
questo tipo. Più nello specifico:
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici1}
  \begin{matrix}
    V\to W\\
    \dim(V)=n\\
    \dim(W)=m\\
    L_A:\mathds{K}^n\to\mathds{K}^m
  \end{matrix} & f
                 \begin{pmatrix}
                   x_1\\
                   x_2\\
                   \vdots\\
                   x_n
                 \end{pmatrix}=
                 \begin{pmatrix}
                   a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n\\
                   a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n\\
                   \vdots\\
                   a_{m1}x_1+a_{m2}x_2+\cdots+a_{mn}x_n
                 \end{pmatrix}
\end{eqnarray}
Quindi dopo aver adattato la formula (\ref{eq:mtxAsaplin7}) alla $f=L_A:\mathds{K}^n\to \mathds{K}^m$, bisogna adattarla
alla $g=L_A:\mathds{K}^p\to \mathds{K}^n$
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici2}
  \begin{matrix}
    Z\to V\\
    \dim(Z)=P\\
    L_B=\mathds{K}^P\to \mathds{K}^n
  \end{matrix}
  & g
  \begin{pmatrix}
    y_1\\
    y_2\\
    \cdots\\
    x_n
  \end{pmatrix}=
  \begin{pmatrix}
    b_{11}y_1+a_{12}y_2+\cdots +a_{1n}x_n\\
    b_{21}y_2+a_{22}y_2+\cdots+ a_{2n}x_n\\
    \cdots\\
    a_{m1}x_1+a_{m2}x_2+\cdots+ a_{mn}x_n
  \end{pmatrix}
\end{eqnarray}
Quindi, in sostanza, è possibile dire che:
\begin{eqnarray*}
  f\circ g: & \mathds{K}^P\overset{\color{red}g}{\to} \mathds{K}^n\overset{\color{red}f}{\to}
              \mathds{K}^m & A \text{ matrice $m\times n$ associata ad } f\\
            & y\to g(y)\to f(g(y)) &  B \text{ matrice $n\times p$ associata ad } g\\
  &&  AB \text{ matrice $m\times p$ associata ad } f\circ g
\end{eqnarray*}
In base a quanto appena visto, la composizione $f\circ g$ può essere calcolata in quanto il codominio di
$g$, ovvero $\mathds{K}^n$, è anche il dominio di $f$, Si ha, quindi
\begin{equation}
  \label{eq:Compinveeproddimatrici3}
  \begin{matrix}
    (f\circ g)
    \begin{pmatrix}
      y_1\\
      y_2\\
      \vdots\\
      y_p
    \end{pmatrix}=f
    \begin{pmatrix}
      b_{11}y_1+b_{12}y_2+\cdots+b_{1p}y_p\\
      b_{21}y_1+b_{22}y_2+\cdots+b_{2p}y_p\\
      \cdots\\
      b_{n1}y_1+b_{n2}y_2+\cdots+b_{np}y_p
    \end{pmatrix}=\\
    =\begin{pmatrix}
      a_{11}(b_{11}y_1+b_{12}y_2+\cdots+b_{1p}y_p)+\cdots+a_{1n}(b_{11}y_1+b_{12}y_2+\cdots+b_{np}y_p)\\
      a_{21}(b_{11}y_1+b_{12}y_2+\cdots+b_{1p}y_p)+\cdots+a_{2n}(b_{11}y_1+b_{12}y_2+\cdots+b_{np}y_p)\\
      \vdots\\
      a_{m1}(b_{11}y_1+b_{12}y_2+\cdots+b_{1p}y_p)+\cdots+a_{mn}(b_{11}y_1+b_{12}y_2+\cdots+b_{np}y_p)\\
    \end{pmatrix}=
  \end{matrix}
\end{equation}
La prossima cosa da fare è raggruppare per $y_1,y_2,\dots,y_p$, ottenendo quindi:
\begin{eqnarray*}
  =\begin{pmatrix}
    (a_{11}b_{11}+\cdots+a_{1n}b_{n1})y_1+(a_{11}b_{12}+\cdots+a_{1n}b_{n2})y_2
    +\cdots+(a_{11}b_{1p}+\cdots+a_{1n}b_{np})y_p\\
    (a_{21}b_{11}+\cdots+a_{1n}b_{n1})y_1+(a_{21}b_{12}+\cdots+a_{1n}b_{n2})y_2
    +\cdots+(a_{21}b_{1p}+\cdots+a_{2n}b_{np})y_p\\
    \vdots\\
    (a_{m1}b_{11}+\cdots+a_{mn}b_{n1})y_1+(a_{m1}b_{12}+\cdots+a_{mn}b_{n2})y_2
    +\cdots+(a_{m1}b_{1p}+\cdots+a_{mn}b_{np})y_p
  \end{pmatrix}
\end{eqnarray*}
Da quest'ultima espressione, è possibile vedere che la composizione $f\circ g$ è ancora una funzione
del tipo (\ref{eq:mtxAsaplin7}), cioè determinata da una matrice $C$, e più precisamente
\begin{equation}
  \label{eq:Compinveeproddimatrici4}
  C=
  \left(
    \begin{array}{cccc}
      a_{11}b_{11}+\cdots+a_{1n}b_{n1}& a_{11}b_{12}+\cdots+a_{1n}b_{n2}& \cdots & a_{11}b_{1p}+\cdots+a_{1n}
                                                                                   b_{np}\\
      a_{21}b_{11}+\cdots+a_{2n}b_{n1} & a_{21}b_{12}+\cdots+a_{2n}b_{n2}& \cdots & a_{21}b_{1p}+\cdots+a_{2n}
                                                                                    b_{np}\\
                                      & \vdots\\
      a_{m1}b_{11}+\cdots+a_{mn}b_{n1} & a_{m1}b_{12}+\cdots+a_{mn}b_{n2} & \cdots & a_{m1}b_{1n}+\cdots+a_{mn}
                                                                                     b_{np}
    \end{array}
  \right)
\end{equation}
Ora, si può notare che le entrate $c_{ij}$ di tale matrice sono tutte espressioni del tipo
\begin{equation}
  \label{eq:Compinveeproddimatrici5}
  c_{ij}=a_{i1}b_{1j}+\cdots+a_{in} b_{nj}
\end{equation}
In parte, è stato definito il prodotto di due matrici $A$ e $B$ in modo che la matrice $C=AB$ ottenuta sia
la matrice che determina la composizione $L_A\circ L_B$ delle funzioni determinate da $A$ e $B$.\\
Ricordando che la composizione $L_A\circ L_B$ delle due funzioni $L_A$ e $L_B$ può essere fatta solo sotto
opportune condizioni\footnote{il codominio di $L_B:\mathds{K}^p\to\mathds{K}^n$ deve essere uguale al dominio
  di $L_A:\mathds{K}^n\to K^m$} si osserva di conseguenza che anche il prodotto di due matrice può essere
fatto solo sotto opportune condizioni: più precisamente, dal momento che la matrice di $L_A$ ha $m$ righe
e $n$ colonne, mentre la matrice $B$ di $L_B$ ha $n$ righe e $p$ colonne, vediamo che si possono moltiplicare
tra loro due matrici $A$ e $B$ (in quest'ordine) se e solo se \textit{il numero di colonne di $A$ è uguale
  al numero di righe di $B$}.
\begin{es}
  \label{es:Compinveeproddimatrici2}
  Come visto nella (\ref{eq:mtxAsaplin10}), la matrice associata alle rotazione $f$ di angolo $\theta$ in
  senso antiorario attorno a $O$ rispetto a una base ortonormale di $V_O^2$ è $A=
  \begin{pmatrix}
    \cos \theta & -\sin\theta\\
    \sin \theta & \cos\theta
  \end{pmatrix}
  $: la funzione $L_A:\mathds{R}^2\to \mathds{R}^2$ determinata da $A$, che manda $(x_1,x_2)$ in
  $(\cos\theta x_1-\sin\theta x_2, \sin\theta x_1+\cos\theta x_2)$, è la traduzione in coordinate di $f$.\\
  Analogamente, se $g$ denota la rotazione di angolo $\phi$, la traduzione in coordinate di $g$ sarà data
  della funzione $L_B$ determinata dalla matrice $B=
  \begin{pmatrix}
    \cos \phi & -\sin\phi\\
    \sin \phi & \cos \phi
  \end{pmatrix}
  $. Dal momento che il prodotto di matrici dà la composizione delle applicazioni corrispondenti, se
  si moltiplicano $A$ e $B$ è possibile ottenere la matrice che rappresenta in coordinate la composizione
  delle due rotazioni: infatti
  \begin{equation*}
    \begin{matrix}
      AB=\begin{pmatrix}
      \cos \theta & -\sin\theta\\
      \sin \theta & \cos\theta
    \end{pmatrix} \begin{pmatrix}
      \cos \phi & -\sin\phi\\
      \sin \phi & \cos \phi
    \end{pmatrix}=\begin{pmatrix}
      \cos \theta \cos \phi -\sin\theta\sin\phi & -\cos\theta\sin\phi-\sin\theta \cos \phi\\
      \sin \theta\cos \phi +\cos\theta\sin\phi & -\sin\theta\sin\phi +\cos\theta \cos \phi
    \end{pmatrix}\\
      =
      \begin{pmatrix}
        \cos(\theta+\phi) & -\sin(\theta+\phi)\\
        \sin(\theta+\phi) & \cos(\theta+\phi)
      \end{pmatrix}
    \end{matrix}
  \end{equation*}
  che è ancora una matrice di rotazione ma relativa al angolo $\theta+\phi$: questo era prevedibile
  in quanto la composizione non fa altro che applicare una rotazione di angolo $\phi$ seguita da una rotazione
  di angolo $\theta$.
\end{es}
\begin{oss}
  \label{oss:Compinveeproddimatrici1}
  Il motivo della non commutatività in generale del prodotto di due matrici $A$ e $B$ è che, tale prodotto
  rappresenta la composizione delle applicazioni $L_A$ e $L_B$ corrispondenti, e la composizione di funzioni
  non gode in generale della proprietà commutativa: ad esempio, date le due funzioni
  \begin{eqnarray*}
    f:\mathds{R}\to \mathds{R},\text{ } f(x)=x+1, & g: \mathds{R}\to\mathds{R}, \text{ } g(x)=x^2
  \end{eqnarray*}
  si ha $f(g(x))=f(x^2)=x^2+1$, mentre, $g(f(x))=g(x+1)=(x+1)^2$, e quindi $f\circ g\neq g\neq f$. Per un
  esempio geometrico, si considerino la funzione $f:V_O^2\to V_O^2$ che ruota ogni vettore del piano applicato
  in $O$ di 90 gradi in senso antiorario, e la funzione $g:V_O^2\to V_O^2$che rifletta ogni vettore attorno a
  una retta $r$ passante per $O$: allora, come si vede nella seguente figura, applicare prima la rotazione $f$
  e poi la riflessione $g$ oppure viceversa porta in generale a risultati diversi (ovvero $f\circ g\neq
  g\circ f$)
  \begin{figure}[ht!]
    \centering
    \resizebox{9cm}{!}{\input{./img/invEprodMatrice4.tikz}}
    \caption{situazione di disegueglianza tra $f\circ g$ e $g\circ f$}
    \label{fig:Compinveeproddimatrici4}
  \end{figure}
\end{oss}
\begin{oss}
  \label{oss:Compinveeproddimatrici2}
  l'associatività del prodotto di matrici si spiega facilmente ricordando che tale prodotto rappresenta la
  composizione delle funzioni corrispondenti, ovvero $(AB)C$ rappresenta $L_A\circ (L_B\circ L_C)$. Ma è facile
  vedere che la composizione di funzioni gode della proprietà associativa: infatti, applicando $(L_A\circ
  L_B)\circ h$ a $x$ si ottiene $(f\circ g)(h(x))=f(g(h(x)))$, e analogamente applicando $f\circ(g\circ h)$ a
  $x$ ottenendo sempre $f((g\circ h)(x))=f(g(h(x)))$, ovvero $(f\circ g)\circ h=f\circ (g\circ h)$.
\end{oss}
Ora, se il prodotto di matrici ammetta un elemento neutro che svolga la stesso ruolo che svolge il numero $1$
per il prodotto tra numeri, per cui si ha $a\cdot 1=1\cdot a =a$ per ogni numero $a$. La risposta è
affermativa: più precisamente, per ogni $n$ considerando la matrice con $n$ righe e $n$ colonne seguente
\begin{equation}
  \label{eq:Compinveeproddimatrici6}
  \begin{pmatrix}
    1 & 0 & \dots & 0\\
    0 & 1 & \dots & 0\\
      && \vdots\\
    0 & 0 & \dots & 1
  \end{pmatrix}
\end{equation}
ovvero la matrice che ha $1$ nelle entrate con stesso indice di riga e di colonne ($a_{11},a_{22}$, etc.) e 0
in tutte le altre entrate. Tale matrice si chiama \textit{matrice identica di ordine $n$} e si denota $I_n$.
Ad esempio,
\begin{eqnarray*}
  I_2=
  \begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}, & I_3=
                   \begin{pmatrix}
                     1 & 0 & 0\\
                     0 & 1 & 0\\
                     0 & 0 & 1
                   \end{pmatrix}.
\end{eqnarray*}
\begin{oss}
  \label{oss:Compinveeproddimatrici3}
  In genere, data una matrice quadrata di ordine $n$, le entrate $a_{11},a_{22},\dots,a_{nn}$ che hanno stesso
  indice di riga e di colonna formano la cosiddetta \textit{diagonale della matrice}. La matrice identica
  $I_n$ può essere quindi descritta come la matrice che ha $1$ sulla diagonale e $0$ nelle altre entrate.
  Le entrate di $I_n$ si denotano solitamente con il simbolo $\delta_{ij}$, detto \textit{delta di
    Kronecker}, che vale quindi $1$ se $i=j$ e 0 se $i\neq j$.\\
  Ora,si può verificare che, per ogni $A\in M_{m,n}(\mathds{K})$ si ha
  \begin{eqnarray*}
    AI_n=A, & I_mA=A
  \end{eqnarray*}
  l'ordine della matrice identica cambia perché deve essere tale che si possa svolgere il prodotto e quindi
  la quindi la matrice identica svolge esattamente il ruolo di elemento neutro per il prodotto righe per
  colonne. Ad esempio,
  \begin{eqnarray*}
    \begin{pmatrix}
      1 & 2 & 3\\
      4 & 5 & 6 
    \end{pmatrix}
    \begin{pmatrix}
      1 & 0 & 0\\
      0 & 1 & 0\\
      0 & 0 & 1
    \end{pmatrix}=
    \begin{pmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{pmatrix}\\
    \begin{pmatrix}
      1 & 0 \\
      0 & 1
    \end{pmatrix}
    \begin{pmatrix}
      1 & 2 & 3\\
      4 & 5 & 6 
    \end{pmatrix}=
    \begin{pmatrix}
      1 & 2 & 3\\
      4 & 5 & 6
    \end{pmatrix}
  \end{eqnarray*}
\end{oss}
\begin{oss}
  \label{oss:Compinveeproddimatrici4}
  Si noti che la matrice identica $I_n$ non è nient'altro che la matrice che determina la funzione identica
  $id_{\mathds{K}^n}:\mathds{K}^n\to\mathds{K}^n$ che manda ogni elemento in se stesso: infatti,
  \begin{equation}
    \label{eq:Compinveeproddimatrici7}
    id_{\mathds{K}^n}
    \begin{pmatrix}
      x_1\\
      x_2\\
      \vdots\\
      x_n
    \end{pmatrix}=
    \begin{pmatrix}
      x_1\\
      x_2\\
      \vdots\\
      x_n 
    \end{pmatrix}=
    \begin{pmatrix}
      1x_1+0x_2+\cdots+0x_n\\
      0x_1+1x_2+\cdots+0x_n\\
      \vdots\\
      0x_1+0x_2+\cdots+1x_n
    \end{pmatrix}
  \end{equation}
  Questo spiega perché tale matrice sia l'elemento neutro per il prodotto, in quanto il prodotto tra matrici
  rappresenta la composizione delle applicazioni corrispondenti e la funzione identica è esattamente
  l'elemento neutro per la composizione.
\end{oss}
Continuando con l'analogia con il prodotto tra numeri, è il caso ora di esaminare la questione
\textit{dell'esistenza dell'inverso}: nell'insieme dei numeri reali $\mathds{R}$ per ogni numero $a$ diverso
d zero esiste un numero $b$ tale che $ab=ba=1$, detto appunto inverso di $a$ (e denotato $a^{-1}$).
Nel caso delle matrici, quindi per capire se data una matrice $A\in M_{m,n}(\mathds{K})$ esiste una
matrice\footnote{La scelta del numero di righe e colonne di $B$ è obbligata se poter eseguire sia il
  prodotto $AB$ che quello $BA$. Una matrice invertibile si può anche chiamare \textsc{Non Singolare}, mentre,
  una matrice non invertibile si può anche chiamare \textsc{Singolare}} $B\in M_{n,m}(\mathds{K})$ tale che
$AB=I_m$ e $BA=I_n$. Tuttavia, se una tale $B$ esiste, dal momento che il prodotto di $A$ e $B$ corrisponde alla
composizione delle applicazioni lineari $L_A:\mathds{K}^n\to\mathds{K}^m$ e $L_B:\mathds{K}^m\to\mathds{K}^n$
mentre la matrice identica corrisponde alla funzione identica, si ha $L_A\circ L_B=id$ e $L_B\circ L_A=id$,
ovvero la funzione $L_A$ deve essere invertibile. Riassumendo, come mostrato, il problema dell'inverso si
pone solo per matrici che hanno stesso numero di righe e colonne, ovvero solo se $A\in M_{n,n}(\mathds{K})$.
Tali matrici si dicono \emph{quadrate} e il numero $n$ comune di righe e colonne si dice \textit{ordine delle
  matrice}. Per semplicità, l'insieme $M_{n,n}(\mathds{K})$ si denota $M_n(\mathds{K})$. 
\begin{defi}
  \label{defi:Compinveeproddimatrici1}
  Una matrice $A\in M_n(\mathds{K})$ quadrata d ordine $n$ si dice \textit{invertibile} se esiste una matrice
  $B\in M_n(\mathds{K})$ tale che
  \begin{eqnarray}
    \label{eq:Compinveeproddimatrici8}
    AB=I_n, & BA=I_n
  \end{eqnarray}
  In tal caso, $B$ si chiama \textit{matrice inversa di} $A$ e si denota $A^{-1}$. Bisogna, dimostrare il
  seguente risultato, contrariamente a quello che accade nel campo dei campo dei numeri reali dove l'unico
  numero non invertibile è lo zero, nell'insieme delle matrici, anche limitandosi alle sole matrici quadrate,
  ci sono molte matrici non invertibili:
\end{defi}
\begin{teo}
  \label{teo:Compinveeproddimatrici1}
  Una matrice $A\in M_n(\mathds{K})$ è invertibile se e solo se il rango di $A$ è uguale a $n$.
\end{teo}
\begin{proof}
  Come ricordato poco fa nella Definizione \ref{defi:Compinveeproddimatrici1}, l'invertibilità di $A$, ovvero
  l'esistenza di una matrice $B$ tale che $AB=BA=I_n$, equivale a dire $L_A\circ L_B=L_B\circ L_A=id$, ovvero
  che $L_A$ è invertibile\footnote{Per essere rigorosi, $L_A\circ L_B=L_B\circ L_A=id$ implica chiaramente che
    $L_A$ sia invertibile, ma viceversa il fatto che $L_A$ sia invertibile implica solo che esista una
    funzione $g$ tale che $g\circ L_A=L_A\circ g=id$ che a priori non si sa se è della forma $g=L_B$ per
    qualche matrice $B$. In realtà, si può dimostrare che $g$ deve necessariamente essere implica che esista
    una matrice $B$ per cui $L_A\circ L_B=L_B\circ L_A=id$ (e quindi $AB=BA=I_n$).} e quindi biiettiva. Quindi
  ci basta dimostrare che $L_A$ è biiettiva se e solo se il rango di $A$ è $n$. Una un funzione lineare in cui
  dominio e codominio abbiano la stessa dimensione (e $L_A:\mathds{K}^n\to \mathds{K}^n$ verifica questa
  condizione) è iniettiva se e solo se è suriettiva, ovvero basta una delle due proprietà per avere anche
  l'altra: in altri termini, per avere entrambe le proprietà
  (e quindi la biiettività) è necessaria e sufficiente una delle due
  proprietà. Quindi, possiamo dire che $L_A$ è biiettiva se e solo
  se e solo se è suriettiva. 
\end{proof}
Vediamo ora come si calcola l'inversa di una matrice $A$.
supponendo che questa esista (cioè che $A$ sia invertibile).

A questo punto siamo pronti a descrivere il primo metodo per la
determinazione dell'inversa di una matrice: grazie all'osservazione
preliminare appera fatta, se si sa che l'inversa di $A$ equivale a
trovare una matrice $B$ tale che $AB=I_n$ (senza dover verificare
anche $BA=I_n$), ovvero
\begin{eqnarray*}
  \begin{pmatrix}
    a_{11} & a_{12} & \dots & a_{1n}\\
    a_{21} & a_{22} & \dots & a_{2n}\\
           && \dots \\
    a_{n1} & a_{n2} &\dots & a_{nn}
  \end{pmatrix}
  \begin{pmatrix}
    b_{11} & b_{12} & \dots & b_{1n}\\
    b_{21} & b_{22} & \dots & b_{2n}\\
           && \dots\\
    b_{n1} & b_{n2} & \dots & b_{nn}
  \end{pmatrix}=
  \begin{pmatrix}
    1 & 0 & \dots & 0\\
    0 & 1 & \dots & 0\\
      && \dots\\
    0 & 0 & \dots & 1
  \end{pmatrix}
\end{eqnarray*}
Tenuto conto della definizione di prodotto righe per colonne,
vediamo che moltiplcando le righe di $A$ per la prima colonna di $B$
deve essere
\begin{eqnarray*}
  \begin{cases}
    a_{11}b_{11} +a_{12}b_{21}+\cdots + a_{1n}b_{n1}=1\\
    a_{21}b_{11} +a_{22}b_{21}+\cdots + a_{2n}b_{n1}=0\\
    \dots\\
    a_{n1}b_{11}+a_{n2}b_{21}+\cdots + a_{nn}b_{n1}=0
  \end{cases}
\end{eqnarray*}
ovvero la prima colonna di $B$ soddisfa il sistema
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici9}
  \begin{cases}
    a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n=1\\
    a_{21}x_{22}x_2+\cdots +a_{2n}x_n=0\\
    \cdots\\
    a_{n1}x_1+a_{n2}x_2+\cdots+a_{nn}x_n=0
  \end{cases}
\end{eqnarray}
con matrice dei coefficienti uguale a $A$ e termini noti uguali alla
prima colonna della matrice identica. Analogamente, moltiplicando
le righe di $A$ per la seconda colonna di $B$ si vede che devono
essere soddisfatte le seguenti
\begin{eqnarray*}
  \begin{cases}
    a_{11}b_{11} +a_{12}b_{21}+\cdots + a_{1n}b_{n2}=0\\
    a_{21}b_{11} +a_{22}b_{21}+\cdots + a_{2n}b_{n2}=1\\
    \dots\\
    a_{n1}b_{11}+a_{n2}b_{21}+\cdots + a_{nn}b_{n2}=0
  \end{cases}
\end{eqnarray*}
ovvero la prima colonna di $B$ soddisfa il sistema
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici10}
  \begin{cases}
    a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n=0\\
    a_{21}x_{22}x_2+\cdots +a_{2n}x_n=1\\
    \cdots\\
    a_{n1}x_1+a_{n2}x_2+\cdots+a_{nn}x_n=0
  \end{cases}
\end{eqnarray}
sempre con matrice dei coefficienti uguale a $A$ ma stavolta con
termini noti uguali alla seconda colonna della matrice identica,
e così via, è possibile ragionare allo stesso modo fino all'ultima
colonna di $B$ che dovrà soddisfare
\begin{eqnarray*}
  \begin{cases}
    a_{11}b_{1n}+a_{12}b_{2n}+\cdots +a_{1n}b_{nn}=0\\
    a_{21}b_{1n}+a_{22}b_{2n}+\cdots+a_{2n}b_{nn}=0\\
    \dots\\
    a_{n1}b_{1n}+a_{n2}b_{2n}+\cdots+a_{nn}b_{nn}=1
  \end{cases}
\end{eqnarray*}
ovvero il sistema
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici11}
  \begin{cases}
    a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n=0\\
  a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n=0\\
  \cdots\\
  a_{n1}x_1+a_{n2}x_2+\cdots+a_{nn}x_n=1
  \end{cases}
\end{eqnarray}
Riassumendo, si ha $AB=I_n$ se e solo se le colonne
\begin{eqnarray*}
  \begin{pmatrix}
    b_{11}\\
    b_{21}\\
    \cdots\\
    b_{nn}
  \end{pmatrix}, &
                   \begin{pmatrix}
                     b_{12}\\
                     b_{22}\\
                     \cdots\\
                     b_{n2}
                   \end{pmatrix}, \text{ } \dots, &
                                            \begin{pmatrix}
                                              b_{1n}\\
                                              b_{2n}\\
                                              \cdots\\

                                              b_{nn}
                                            \end{pmatrix}
\end{eqnarray*}
sono soluzioni rispettivamente degli $n$ sistemi della
(\ref{eq:Compinveeproddimatrici9}),
(\ref{eq:Compinveeproddimatrici10}),
\dots, (\ref{eq:Compinveeproddimatrici11}). Per risolvere un sistema
basta scriverne la matrice completa, composta da matrice dei
coefficienti delle incognite e colonna dei termini noti, e ridurla
a gradini madiante operazioni elementari sulle righe. Poiché i
sistemi (\ref{eq:Compinveeproddimatrici9}),
(\ref{eq:Compinveeproddimatrici10}),
\dots, (\ref{eq:Compinveeproddimatrici11}) hanno tutti la stessa
matrice dei coeffcienti, cioè $A$, e differiscono solo per i
termini noti, è possibile risolverli tutti contemporaneamente
eseguendo la stesse operazioni elementari: a questo scopo, basta
scrivere la matrice
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici12}
  (A| I_n)=\left(
  \begin{array}{cccc|cccc}
    a_{11}& a_{12}& \dots& a_{1n}& 1& 0& \dots&0 \\
    a_{21} & a_{22} & \dots &a_{2n} & 0 & 1 &\dots &0\\
          &&\dots&&&&\dots\\
    a_{n1}& a_{n2} & \dots&a_{nn} & 0 & 0 & \dots & 1
  \end{array}
  \right)
\end{eqnarray}
ottenuta affiancando tutti i termini noti dei sistemi
(\ref{eq:Compinveeproddimatrici9}),
(\ref{eq:Compinveeproddimatrici10}),
\dots, (\ref{eq:Compinveeproddimatrici11}) e risolverli
contemporaneamente con una sola riduzione\footnote{chiaramente,
  in base al Teorema \ref{teo:Compinveeproddimatrici1} la matrice
  $A$ è invertibile se e solo se in seguito a tale riduzione non si
  annullerà nessuna delle sue righe}.

Ad esempio: data la matrice $A=
\begin{pmatrix}
  1 & -1\\
  1 & 2
\end{pmatrix}
$, verificando se essa è invertibile e, in caso affermativo,
calcolando l'inversa. Come detto sopra, affiancando a tale matrice
la matrice identità dello stesso ordine
\begin{eqnarray*}
  (A|I_n)=\left(
  \begin{array}{cc|cc}
    1 & -1 & 1 & 0\\
    1 &  2 & 0 & 1
  \end{array}
  \right)
\end{eqnarray*}
che rappresenta i due sistemi le cui soluzioni le colonne della
matrice inversa, e si inizia con l'applicare il procedimento di
risoluzione a gradini: a questo scopo basta il singolo passaggio
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici13}
  (A|I_n)=\left(
  \begin{array}{cc|cc}
    1 & -1 & 1 & 0 \\
    1 & 2 & 0 & 1
  \end{array}\right) \underset{R_2\to R_2-R_1}{\to}\left(
  \begin{array}{cc|cc}
    1 & -1 & 1 & 0\\
    0 & 3 & -1 & 1
  \end{array}\right)
\end{eqnarray}
da cui si vede che, dopo la riduzione a gradini, ad $A$ non si
annulla nessuna riga e quindi, come detto sopra, $A$ è
invertibile.\\
La prima colonna dell'inversa $B$ in $A$ è data dalla soluzione del
sistema ridotto
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici14}
  \begin{cases}
    x_1-x_2=1\\
    3x_2=-1
  \end{cases}
\end{eqnarray}
ovvero, come si vede risolvendo dal basso, la coppia
$\left(\frac{2}{3},-\frac{1}{3}\right)$, che è quindi la prima
colonna della matrice inversa. Analogamente, la seconda colonna
dell'inversa $B$ di $A$ è data dalla soluzione del sistema ridotto
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici15}
  \begin{cases}
    x_1-x_2=0\\
    3x_2=1
  \end{cases}
\end{eqnarray}
ovvero, come si vede risolvendo dal basso, la coppia
$\left(\frac{1}{3},\frac{1}{3}\right)$, che è quindi la seconda
colonna della matrice inversa. In conclusione, l'inversa della
matrice $A$ è
\begin{equation*}
  A^{-1}=
  \begin{pmatrix}
    \frac{2}{3} & \frac{1}{3}\\
    -\frac{1}{3} & \frac{1}{3}
  \end{pmatrix}
\end{equation*}
Infatti, si verifica subito con un calcolo che
\begin{equation*}
  \begin{pmatrix}
    1 & -1\\
    1 & 2
  \end{pmatrix}
  \begin{pmatrix}
    \frac{2}{3} & \frac{1}{3}\\
    -\frac{1}{3} & \frac{1}{3}
  \end{pmatrix}=\begin{pmatrix}
    \frac{2}{3} & \frac{1}{3}\\
    -\frac{1}{3} & \frac{1}{3}
  \end{pmatrix}\begin{pmatrix}
    1 & -1\\
    1 & 2
  \end{pmatrix}=
  \begin{pmatrix}
    1 & 0\\
    0 & 1
  \end{pmatrix}
\end{equation*}
concordemente con la definizione di inversa.
Per evitare di scrivere e risolvere separatamente i sistemi
(\ref{eq:Compinveeproddimatrici14}) e 
(\ref{eq:Compinveeproddimatrici15}) e trovare invece, in modo
più diretto la matrice inversa, si può procedere come segue. Dopo
aver effettuato la riduzione a gradini in
(\ref{eq:Compinveeproddimatrici13}), si applicano ulteriori
operazioni elementari fino a trasformare la matrice $A$ del blocco
legge direttamente l'inversa. Per vederlo, bisogna riprendere da 
(\ref{eq:Compinveeproddimatrici13}) e facciamo comparire prima
uno zero in posizione 1 2 eseguendo
\begin{eqnarray*}  
  \left(
  \begin{array}{cc|cc}
    1 & -1 & 1 & 0 \\
    0 & 3 & -1 & 1
  \end{array}\right) \underset{R_1\to 3R_1+R_2}{\to}\left(
  \begin{array}{cc|cc}
    3 & 0 & 2 & 1\\
    0 & 3 & -1 & 1
  \end{array}\right)
\end{eqnarray*}
e poi bisogna applicare a ogni riga l'operazione elementare del
secondo tipo che consiste nel dividerla per l'elemento che si
trovare sulla diagonale:
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici16}
  \left(\begin{array}{cc|cc}
    3 & 0 & 2 & 1\\
    0 & 3 & -1 & 1
  \end{array}\right) \underset{
  \begin{matrix}
    R_1=\left(\frac{1}{3}\right) R_1\\
    R_2=\left(\frac{1}{3}\right) R_2
  \end{matrix}
  }{\to}\left(
  \begin{array}{cc|cc}
    1 & 0 & \frac{2}{3} & \frac{1}{3}\\
    0 & 1 & -\frac{1}{3}& \frac{1}{3}
  \end{array}\right)
\end{eqnarray}
Come si vede dalla matrice identica che è stata affiancata ad $A$
si è trasformata nella matrice inversa di $A$ già trovata sopra.\\
Per capire il perché, bisogna ricorcare che le operazioni elementari
che vengono qui svolte servono a risolvere contemporaneamente i
sistemi che poi danno come soluzione le colonne della matrice
inversa: ma allora, riducendo la matrice $A$ alla matrice identica
come in (\ref{eq:Compinveeproddimatrici16}) non si sta facendo altro
che ridurre i due sistemi alla forma
\begin{eqnarray*}
  \begin{cases}
    x_1=\frac{2}{3}\\
    x_2=-\frac{1}{3}
  \end{cases}, &
                 \begin{cases}
                   x_1=\frac{1}{3}\\
                   x_2=\frac{1}{3}
                 \end{cases}
\end{eqnarray*}
e cioè far comparire direttamente le soluzioni cercate (\emph{che sono
proprio le colonne della matrice inversa}).

Un altro esempio:
\begin{equation*}
  A=
  \begin{pmatrix}
    1 & 1 & 2\\
    -1 & 1 & 0\\
    2 & 1 & 1
  \end{pmatrix}.
\end{equation*}
La prima cosa da fare è quella di trasformare $(A|I_n)$ in una matrice a gradini, come
prima andando ad affiancare la matrice per la sua identità:
\begin{eqnarray*}
  (A|I_n)=\left(
  \begin{array}{ccc|ccc}
    1 & 1 & 2 & 1 & 0 & 0\\
    -1 & 1 & 0 & 0 & 1 & 0\\
    2 & 1 & 1 & 0 & 0 & 1
  \end{array}
  \right)\underset{
  \begin{matrix}
    R_2\to R_2 +R_1\\
    R_3\to R_3-2R_1
  \end{matrix}
  }{\to}\left(
  \begin{array}{ccc|ccc}
    1 & 1 & 2 & 1 & 0 & 0\\
    0 & 2 & 2 & 1 & 1 & 0\\
    0 & -1 & -3 & -2 & 0 & 1
  \end{array}
  \right)\\
  \underset{R_3\to 2R_3+R_2}{\to}
  \left(
  \begin{array}{ccc|ccc}
    1 & 1 & 2 & 1& 0&0\\
    0 & 2 & 2 & 1 & 1 & 0\\
    0 & 0 & -4 & -3 & 1 & 2
  \end{array}
  \right)
\end{eqnarray*}
Il fatto che non si sia annullata nessuna riga nel blocco di sinistra dice che la matrice $A$ è
invertibile.

Ora, come spiagato sompra, bisogna far comparire zeri sopra la diagonale, effettuando una sorta di
riduzione a gradini ``all'incontrario'', dal basso verso l'alto e destra verso sinistra\footnote{
  in ogni passaggio, bisogna mettere in evidenza in grassetto i nuovi zeri che si fanno comparire}
\begin{eqnarray*}
  \left(
  \begin{array}{ccc|ccc}
    1 & 1 & 2 & 1 & 0 & 0\\
    0 & 2 & 2 & 1 & 1 & 0\\
    0 & 0 & -4 & -3 & 1 & 2
  \end{array}\right) \underset{
  \begin{matrix}
    R_2\to R_2+R_3\\
    R_1\to 3R_1+R_3
  \end{matrix}
  }{\to}\left(
  \begin{array}{ccc|ccc}
    2 &2 & 0 & -1 & 1 & 2 \\
      0 & 4 &0 &-1 &3 &2 \\
      0 & 0 & -4 & -3 & 1&2
  \end{array}
  \right)\\
  \underset{
  R_1\to R_1-R_2
  }{\to}\left(
  \begin{array}{ccc|ccc}
    4 & 0 & 0 & -1 & -1 & 2 \\
    0 & 4 & 0 & -1 & 3 & 2 \\
    0 & 0 & -4 & -3 & 1 & 2 
  \end{array}
  \right)
\end{eqnarray*}
Infine, si divide ogni riga per l'elemento sulla diagonale applicando operazioni elementari
del secondo tipo
\begin{eqnarray*}
  \left(
  \begin{array}{ccc|ccc}
    4 & 0 & 0 & -1 & -1 & 2 \\
    0 & 4 & 0 & -1 & 3 & 2 \\
    0 & 0 & -4 & -3 & 1 & 2 
  \end{array}
  \right)\underset{
  \begin{matrix}
    R_1\to \frac{1}{4}R_1\\
    R_1\to \frac{1}{4}R_2\\
    R_3\to -\frac{1}{4}R_3
  \end{matrix}
  }{\to} \left(
  \begin{array}{ccc|ccc}
    1 & 0 & 0 & -\frac{1}{4} &-\frac{1}{4}&\frac{1}{2}\\
    0 & 1 & 0 & -\frac{1}{4} & \frac{3}{4} & \frac{1}{2}\\
    0 & 0 & 1 & \frac{3}{4} & -\frac{1}{4} & -\frac{1}{2}
  \end{array}
  \right)
\end{eqnarray*}
Quindi
\begin{equation*}
  A^{-1}=
  \begin{pmatrix}
    -\frac{1}{4} & -\frac{1}{4} & \frac{1}{2}\\
    -\frac{1}{4} & \frac{3}{4} & \frac{1}{2}\\
    \frac{3}{4} & -\frac{1}{4} & -\frac{1}{2}
  \end{pmatrix}.
\end{equation*}
Un modo alternativo per calcolare l'inversa di una matrice invertibile, basato sul determinante e sulla
notizoa di cofattore. Come visto nel Teorema \ref{teo:Compinveeproddimatrici1} che una matrice $A$ di
ordine $n$ è invertibile se e solo se il suo rango è $n$. Ma questo equivale ad avere $\det (A)\neq 0$.
Quindi, è necessario dimostrare la seguente
\begin{prop}
  \label{prop:Compinveeproddimatrici3}
  Sia $A\in M_n(\mathds{K})$ una matrice invertibile (ovvero con $\det(A)\neq 0$). Allora la sua
  inversa $A^{-1}$ è data da
  \begin{equation}
    \label{eq:Compinveeproddimatrici17}
    A^{-1}=\frac{1}{\det(A)}
    \begin{pmatrix}
      C_{11} & C_{21} & \dots & C_{n1}\\
      C_{12} & C_{22} & \dots & C_{n2}\\
             & \vdots\\
      C_{1n} & C_{2n} &\dots & C_{nn}
    \end{pmatrix}
  \end{equation}
  dove $C_{ij}$ indica il cofattore di $a_{ij}$, e $\frac{1}{\det(A)}$ davanti alla matrice dei cofattori
  significa che ogni entrata di tale matrice deve essere moltiplicata per $\frac{1}{\det(A)}$.
\end{prop}
\begin{oss}
  \label{oss:Compinveeproddimatrici4}
  A proposito delle disposizione dei cofattori nella (\ref{eq:Compinveeproddimatrici17}), si notiche i
  cofattori delle entrate della prima \textit{riga} di $A$ sono nella prima \textit{colonna} della
  (\ref{eq:Compinveeproddimatrici17}), i cofattori delle entrate della seconda \textit{riga} di $A$
  sono nella seconda \textit{colonna} della (\ref{eq:Compinveeproddimatrici17}), e così via.
\end{oss}
\begin{es}
  \label{es:Compinveeproddimatrici2}
  Calcolare l'inversa della matrice $A=
  \begin{pmatrix}
    1 & 2 & 1\\
    1 & -1 & 1 \\
    1 & 0 & 2
  \end{pmatrix}
  $.
  I cofattori sono
  \begin{eqnarray*}
    C_{11}=(-1)^{1+1}\det
    \begin{pmatrix}
      -1 & 1 \\
      1 & 2
    \end{pmatrix}=-2,&C_{12}=(-1)^{1+2}\det
                       \begin{pmatrix}
                         1 & 1\\
                         1 & 2
                       \end{pmatrix}=-1\\
    C_{13}=(-1)^{1+3}\det
    \begin{pmatrix}
      1 & -1\\
      1 & 0
    \end{pmatrix}=1, & C_{21}=(-1)^{2+1}\det
                       \begin{pmatrix}
                         2 & 1\\
                         0 & 2
                       \end{pmatrix}=-4\\
    C_{22}(-1)^{2+2}\det
    \begin{pmatrix}
      1 & 1\\
      1 & 2
    \end{pmatrix}=1, &C_{23}=(-1)^{2+3}\det
                       \begin{pmatrix}
                         1&2\\
                         1 & 0
                       \end{pmatrix}=2\\
    C_{31}=(-1)^{3+1}\det
    \begin{pmatrix}
      2 & 1\\
      -1 & 1
    \end{pmatrix}=3, & C_{32}=(-1)^{3+2}\det
                       \begin{pmatrix}
                         1 & 1\\
                         1 & 1
                       \end{pmatrix}=0\\
    C_{33}=(-1)^{3+3}\det
    \begin{pmatrix}
      1 & 2\\
      1 & -1
    \end{pmatrix}=-3.
  \end{eqnarray*}
  Quindi
  \begin{equation*}
    A^{-1}=\frac{1}{\det(A)}
    \begin{pmatrix}
      C_{11} & C_{21} & \dots & C_{31}\\
      C_{12} & C_{22} & \dots & C_{32}\\
             & \vdots\\
      C_{13} & C_{23} &\dots & C_{33}
    \end{pmatrix}=-\frac{1}{3}
    \begin{pmatrix}
      -2 & -4 & 3\\
      -1 & 1 & 0\\
      1 & 2 & -3
    \end{pmatrix}=
    \begin{pmatrix}
      \frac{2}{3} & \frac{4}{3} & -1\\
      \frac{1}{3} & -\frac{1}{3} & 0\\
      -\frac{1}{3}&\frac{2}{3} & 1
    \end{pmatrix}.
  \end{equation*}
  dove il determinanmte di $A$ è stato calcolato sviluppandolo secondo Laplace rispetto alla
  terza riga, usando i cofattori già calcolati:
  \begin{equation*}
    \det(A)=a_{31}C_{31}+a_{32}C_{32}+a_{33}C_{33}=1\cdot 3+0\cdot 0+2\cdot (-3)=-3
  \end{equation*}
\end{es}\clearpage
\begin{oss}
  \label{oss:Compinveeproddimatrici5}
  Osservando che la (\ref{eq:Compinveeproddimatrici17}), nel caso $n=2$, diventa la semplice formula
  \begin{equation}
    \label{eq:Compinveeproddimatrici18}
    \begin{pmatrix}
      a_{11} & a_{12}\\
      a_{21} & a_{22}
    \end{pmatrix}^{-1}=\frac{1}{a_{11}a_{22}-a_{12}a_{21}}
    \begin{pmatrix}
      a_{22}&-a_{12}\\
      -a_{21} & a_{11}
    \end{pmatrix}.
  \end{equation}
  in quanto i cofattori sono semplicemente $C_{11}=(-1)^{1+1}a_{22},C_{12}=(-1)^{1+2}a_{21}, C_{21}=(-1)^{2+1}a_{12},$ \\$C_{22}=(-1)^{2+2}a_{11}$, ricordando che il determinante di una metrice di
  ordine 1 si definisce come il valore della sua unica entrata. In pratica, a parte dividere per
  il determinante, la matrice inversa si ottiene scambiando tra loro i due elementi sulla diagonale
  e cambiando di segno le restanti entrate.
\end{oss}
\begin{es}
  \label{es:Compinveeproddimatrici3}
  Applicando la (\ref{eq:Compinveeproddimatrici18}) al caso delle caso della matrice che rappresenta una
  rotazione di angolo $\theta$ nel piano, ovvero $A=
  \begin{pmatrix}
    \cos \theta & -\sin \theta\\
    \sin\theta & \cos \theta
  \end{pmatrix}
  $: si ha
  \begin{equation*}
    A^{-1}=\frac{1}{\cos^2\theta+\sin^2\theta}
    \begin{pmatrix}
      \cos\theta & \sin\theta\\
      -\sin\theta & \cos\theta
    \end{pmatrix}=
  \end{equation*}
  per l'identità $\cos^2\theta+\sin^2\theta=1$ e le formule trigonometriche per il seno e il coseno
  dell'opposto di un angolo
  \begin{equation*}
    =
    \begin{pmatrix}
      \cos(-\theta) & -\sin(-\theta)\\
      \sin(-\theta) & \cos(-\theta)
    \end{pmatrix}
  \end{equation*}
  che è ancora una matrice di rotazione, quella associata alla rotazione di angolo $-\theta$, che è
  l'inversa della rotazione di angolo $\theta$.
  
  In effetti, in quenere, la matrice $A^{-1}$ inversa di una matrice $A$ data rappresenta la funzione
  inversa della funzione $L_A:\mathds{K}^n\to \mathds{K}^n$ determinante determinata da $A$: infatti,
  come sapendo la composizione $L_A\circ L_{A^{-1}}$ è la funzione determinata dal prodotto $AA^{-1}=I_n$,
  ovvero la funzione identica, cioè $L_A\circ L_{A^{-1}}=id$ (analogamente $L_{A^{-1}}\circ L_A=id$).
\end{es}
La formula (\ref{eq:Compinveeproddimatrici17}) per il calcolo dell'inversa può essere utilizzata per
ricavare un modo alternativo alla riduzione a gradini per risolvere certi sistemi di equazioni lineari.

A tale scopo, osservando prima che un generico sistema lineare di $m$ equazioni in $n$ incognite
\begin{equation}
  \label{eq:Compinveeproddimatrici19}
  \begin{cases}
    a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n=b_1\\
    a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n=b_2\\
    \vdots\\
    a_{m1}x_1+a_{m2}x_{2}+\cdots+a_{mn}x_n=b_m
  \end{cases}
\end{equation}
può essere riscritto in forma molto più concisa grazie al prodotto di matrici. Più precisamente, se
si denota con
\begin{equation*}
  A=
  \begin{pmatrix}
    a_{11}&a_{12}&\cdots &a_{1n}\\
    a_{21}&a_{22}&\cdots & a_{2n}\\
          & \vdots\\
    a_{m1} & a_{m2} & \cdots & a_{mn}
  \end{pmatrix}
\end{equation*}
la matrice dei coefficienti del sistema, con $x=
\begin{pmatrix}
  x_1\\
  x_2\\
  \vdots\\
  x_n
\end{pmatrix}$ la matrice\footnote{costituita da una sola colonna} che ha come entrate le incognite,
e con $b=
\begin{pmatrix}
  b_1\\
  b_2\\
  \vdots\\
  b_m
\end{pmatrix}$ la matrice, sempre costituita da una sola colonna, che ha come entrate i termini noti del
sistema si ha, svolgendo il prodotto righe per colonne, che
\begin{equation*}
  \begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    a_{21} & a_{22} & \cdots & a_{2n}\\
           &\vdots\\
    a_{m1} & a_{m2} & \cdots & a_{mn}
  \end{pmatrix}
  \begin{pmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_n
  \end{pmatrix}=
  \begin{pmatrix}
     a_{11}x_1+a_{12}x_2+\cdots+a_{1n}x_n\\
    a_{21}x_1+a_{22}x_2+\cdots+a_{2n}x_n\\
    \vdots\\
    a_{m1}x_1+a_{m2}x_{2}+\cdots+a_{mn}x_n
  \end{pmatrix}
\end{equation*}
E quindi il sistema (\ref{eq:Compinveeproddimatrici19}) può essere riscritto
\begin{equation*}
  \begin{pmatrix}
    a_{11} & a_{12} & \cdots & a_{1n}\\
    a_{21} & a_{22} & \cdots & a_{2n}\\
           &\vdots\\
    a_{m1} & a_{m2} & \cdots & a_{mn}
  \end{pmatrix}
  \begin{pmatrix}
    x_1\\
    x_2\\
    \vdots\\
    x_n
  \end{pmatrix}=\begin{pmatrix}
    b_1\\
    b_2\\
    \vdots\\
    b_m
  \end{pmatrix}
\end{equation*}
ovvero nella semplice forma
\begin{equation}
  \label{eq:Compinveeproddimatrici20}
  Ax=b.
\end{equation}
Supponendo che la matrice $A$ dei coefficienti del sistema sia quadrata di ordine $n$ (quindi il
sistema ha equazioni e $n$ incognite) e che abbia determinante diverso da zero. Allora $A$ invertibile
ed è possibile moltiplicare entrambi i membri dell'uguaglianza (\ref{eq:Compinveeproddimatrici20}) a
sinistra per l'inversa $A^{-1}$:
\begin{equation*}
  A^{-1}(Ax)=A^{-1}b
\end{equation*}
Tenendo conto che il prodotto di matrici gode della proprietà associativa, questo può essere riscritto
come
\begin{equation*}
  (A^{-1}A)x=A^{-1}b
\end{equation*}
ovvero, visto che per definizione di inversa $A^{-1}A=I_n$, $I_nx=A^{-1}b$ e quindi, essendo $I_n$
elemento neutro per il prodotto,
\begin{equation}
  \label{eq:Compinveeproddimatrici21}
  x=A^{-1}b
\end{equation}
Quindi la soluzione $x$ di un sistema di equazioni e di incognite e matrice dei coefficienti con
determinante diverso da zero può essere trovata mediante la (\ref{eq:Compinveeproddimatrici21}).
\begin{oss}
  \label{oss:Compinveeproddimatrici6}
  Si noti che non si è fatto altro che applicare gli stessi passaggi, normalmente sottointesi, che
  si applicano quando si vuole risolvere una semplice equazione di primo grado in una sola incognita
  $ax=b$. Infatti, ad esempio, se bisogna risolvere $2x=3$, basta dividere entrambi i membri per $2$,
  ovvero equivalentemente moltiplicando per l'inverso $\frac{1}{2}$ di 2 ottenendo
  $\frac{1}{2}(2x)=\frac{1}{2}3=\frac{3}{2}$; per la proprietà associativa a primo membro si ha
  $(\frac{1}{2}2)x=\frac{3}{2}$ ovvero, essendo $\frac{1}{2}2=1$, si ha $1x=\frac{3}{2}$ cioè, essendo 1
  elemento neutro per la moltiplicazione tra numeri, $x=\frac{3}{2}$, che è la soluzione dell'equazione.

  L'unica differenza con il caso dei sistemi $Ax=b$ è che non sempre $A$ è invertibile, mentre, a meno
  che $a$ non sia zero, nel equazione $ax=b$ tale ipotesi è sempre garantita.

  Se si combina la (\ref{eq:Compinveeproddimatrici21}) con la formula per l'inverso
  (\ref{eq:Compinveeproddimatrici14}), si vede che la soluzione di un sistema con $n$ equazioni e $n$
  incognite, e matrice dei coefficianti invertibile è data da
  \begin{equation}
    \label{eq:Compinveeproddimatrici22}
    \begin{pmatrix}
      x_{1}\\
      x_2\\
      \vdots\\
      x_n
    \end{pmatrix}=\frac{1}{\det(A)}
    \begin{pmatrix}
      C_{11} & C_{21} & \cdots & C_{n1}\\
      C_{12} & C_{22} & \cdots & C_{n2}\\
             && \vdots \\
      C_{1n} & C_{2n} & \cdots & C_{nn}
    \end{pmatrix}
    \begin{pmatrix}
      b_1\\
      b_2\\
      \vdots\\
      b_n
    \end{pmatrix}.
  \end{equation}
\end{oss}
Quindi, come si vede, le componenti $x_i$ della soluzione del sistema si ottengono moltiplicando le
righe della matrice dei cofattori (\textit{divisa per il determinante di $A$}) per la colonna dei
termini noti:
\begin{eqnarray*}
  x_1=\frac{b_1C_{11}+b_2C_{21}+\cdots+b_nC_{n1}}{\det(A)}\\
  x_2=\frac{b_1C_{12}+b_2C_{22}+\cdots+b_nC_{n2}}{\det(A)}\\
  \vdots
\end{eqnarray*}
e in genere
\begin{eqnarray}
  \label{eq:Compinveeproddimatrici23}
  x_i=\frac{b_1C_{1i}+b_2C_{2i}+\cdots+b_nC_{ni}}{\det(A)}
\end{eqnarray}
Se si confronta il numeratore della (\ref{eq:Compinveeproddimatrici23}) con l'espressione del
determinante del determinante di $A$ calcolato con lo sviluppo di Laplace rispetto alla $i$-esima colonna
\begin{equation*}
  \det(A)=a_{1i}C_{1i}+a_{2i}C_{2i}+\cdots+a_{ni}C_{ni}
\end{equation*}
si vedrà che l'unica differenza è che al posto degli elementi $a_{1i},a_{2i},\dots,a_{ni}$ delle colonna
$i$-esima di $A$ si hanno i termini noti $b_1,b_2,\dots,b_n$: in altri termini, tale numeratore coincide
con lo sviluppo di Laplace del determinante della matrice che si ottiene da $A$ sostituendo i termini
noti al posto della $i$-esima colonna.

Riassumendo, è stato dimostrato il seguente
\begin{teo}
  \label{teo:Compinveeproddimatrici2}
  Sia $Ax=b$ un sistema di $n$ equazioni lineari in $n$ incognite, con $\det(A)\neq 0$ (cioè $A$ è
  invertibile). Allora tale sistema ha un'unica soluzione $(x_{1},x_2,\dots,x_n)$ le cui componenti
  sono date da
  \begin{eqnarray*}
    x_i=\frac{\bf \det(B_i)}{\det (A)}, & i=1,\dots,n
  \end{eqnarray*}
  dove $B_i$ è la matrice che si ottiene da $A$ sostituendo la colonna dei termini noti $b$ al posto
  della $i$-esima colonna di $A$. Questo viene chiamato Teorema di Cramer
\end{teo}
\begin{es}
  \label{es:Compinveeproddimatrici5}
  Considerando il sistema
  \begin{equation*}
    \begin{cases}
      2x_1+x_2=5\\
      x_1-x_2=3
    \end{cases}
  \end{equation*}
  La matrice $A=
  \begin{pmatrix}
    2 & 1 \\
    1 & -1
  \end{pmatrix}$ dei coefficienti del sistema ha determinante $\det(A)=-3$, quindi è invertibile e
  si può applicare il metodo di Cramer, ovvero si hanno le componenti dell'unica soluzione date da
  \begin{eqnarray*}
    x_1=\frac{\det(B_1)}{\det(A)}=\frac{\det
    \begin{pmatrix}
      5 & 1\\
      3 & -1
    \end{pmatrix}
    }{\det(A)}=\frac{-8}{-3}=\frac{8}{3}\\
    x_2=\frac{\det(B_2)}{\det(A)}=\frac{\det
    \begin{pmatrix}
      2 & 5\\
      1 & 3
    \end{pmatrix}
    }{\det(A)}=\frac{1}{-3}=-\frac{1}{3}
  \end{eqnarray*}
  Per concludere questa parte sull'inversa, mostrando che, data una matrice invertibile $A$, vale
  la seguente
  \begin{equation}
    \label{eq:Compinveeproddimatrici24}
    \det(A^{-1})=\frac{1}{\det(A)}
  \end{equation}
  Questa uguaglianza si dimostra come corollario del seguente, importante risultato, detto
  \textit{teorema di Binet}:
\end{es}
\begin{teo}
  \label{es:Compinveeproddimatrici6}
  Siano $A,B\in M_n(\mathds{K})$ due matrici quadrate. Allora
  \begin{equation*}
    \det(AB)=\det(A)\det(B)
  \end{equation*}
  Non si dimostra il teorema di Binet, ma si illustra con un esempio: se $A=
  \begin{pmatrix}
    1 & 2\\
    3 & 4
  \end{pmatrix}$ e $B=
  \begin{pmatrix}
    2 & 1\\
    1 & 3
  \end{pmatrix}$, allora $AB=
  \begin{pmatrix}
    4 & 7\\
    10 & 15
  \end{pmatrix}$, e si vede che $\det(A)=-2, \det(B)=5$ e $\det(AB)=-10=(-2)\cdot 5$, concordemente con
  il teorema di \textit{Binet}.
\end{teo}
Per Dimostrare la (\ref{eq:Compinveeproddimatrici24}), basta applicare il teorema al Binet al caso
$B=A^{-1}$:
\begin{equation*}
  \det(AA^{-1}=\det(A)\det(A^{-1})
\end{equation*}
ovvero, tenuto conto che $AA^{-1}=I_n$,
\begin{equation}
  \label{eq:Compinveeproddimatrici25}
  \det(I_n)=\det(A)\det(A^{-1})
\end{equation}
Ora, è facile vedere che il determinante della matrice identica $I_n$, per qualunque ordine $n$,
è uguale a $1$: infatti, basta applicare lo sviluppo di Laplace rispetto alla prima riga.

Quindi la (\ref{eq:Compinveeproddimatrici25}) diventa
\begin{equation*}
  1=\det(A)\det(A^{-1})
\end{equation*}
che implica subito la (\ref{eq:Compinveeproddimatrici24}).

Si presti comunque attenzione che la non commutatività del prodotto di matrici rende non valide
alcune identità classiche dell'algebra numerica, come la formula per il quadrato di un binomio
\begin{equation*}
  (a+b)^2=a^2+2ab+b^2
\end{equation*}
o il prodotto notevole $(a+b)(a-b)=a^2-b^2$. Infatti, per matrici si ha
\begin{equation*}
  (A+B)=(A+B)(A+B)=A^2+AB+BA+B^2
\end{equation*}
Nella seconda uguaglianza si ha usato la proprietà distributiva, che vale anche per matrici, ma
non valendo la commutatività del prodotto non può essere scritto come $AB+BA=2AB$.

Analogamente, $(A+B)(A-B)=A^2-AB+BA-B^2$ e di nuovo non essendo valida la commutatività del prodotto di
matrici non si può semplicemente fare $-AB+BA$.

\subsection{Ultime proprietà della matrice inversa}
\label{sec:propmatinv}
Per completezza è giusto aggiungere due proprietà che in questo capitolo, fino ad ora non sono state
citate, sostanzialmente si tratta di due uguaglianze:
\begin{itemize}
\item $(A^{-1})^{-1}=A$;
\item $(AB)^{-1}=B^{-1}A^{-1}$.
\end{itemize}
\begin{proof}
  Quindi, partendo dalla prima delle due affermazioni, bisogna partire dalla seguente situazione:
  \begin{eqnarray*}
    (AB)(B^{-1}A^{-1})\overset{?}{=}I\\
    A(BB^{-1})A^{-1}\\
    AIA^{-1}\\
    AA^{-1}=I
  \end{eqnarray*}
  Mentre, per il secondo punto, la situazione di partenza risulta la seguente:
  \begin{eqnarray*}
    (B^{-1}A^{-1})(AB)\overset{?}{=}I\\
    B^{-1}(A^{-1}A)B\\
    B^{-1}IB\\
    B^{-1}B=I
  \end{eqnarray*}
  e con questo si sono dimostrati i due punti trattati.
\end{proof}
